{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b75aa37-05ff-4362-a41d-453bad2ccdd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/51/51/b87caa939fedf307496e4dbf412f4b909af3d9ca8b189fc3b65c1faa456f/transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.0 MB 3.0 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting datasets\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ed/a5/33cf000137545a08b0a3a6ea76c8ccbd87917f78bb5d737f9f56f3b11ef6/datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 480 kB 24.8 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/cb/bd/1a875e0d592d447cbc02805fd3fe0f497714d6a2583f59d14fa9ebad96eb/huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 566 kB 26.2 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/5a/c8/dc7153ceb5bcc344f5c4f0291ea45925a5f00009afa3849e91561ac2e847/regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 785 kB 20.9 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.8/site-packages (from transformers) (3.10.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (4.61.2)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 471 kB 73.8 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting tokenizers<0.21,>=0.20\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/02/52/cd7b83b6e0a1fda503ca7184b0162583de6d2f176dda9aa02abf80cb247b/tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.0 MB 53.3 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/9a/92/cf3ab0b652b082e66876d08da57fcc6fa2f0e6c70dfbbafbd470bb73eb47/hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3 MB 77.3 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/56/53/eb690efa8513166adef3e0669afd31e95ffde69fb3c52ec2ac7223ed6018/fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 193 kB 57.8 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.5.0)\n",
      "Collecting requests\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/7c/e4/56027c4a6b4ae70ca9de302488c5ca95ad4a39e190093d6c1a8ace08341b/requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001B[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ea/89/38df130f2c799090c978b366cfdf5b96d08de5b29a4a293df7f7429fa50b/multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 132 kB 42.0 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/1d/a0/6aaea0c2fbea2f89bfd5db25fb1e3481896a423002ebe4e55288907a97a3/fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179 kB 66.4 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting tqdm>=4.27\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78 kB 16.2 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting xxhash\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/fc/17/b27a42b984ed3fca58aba8b2dd2b77ae44aa1331d8e1c19bc50e1eb055c2/xxhash-3.6.0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 193 kB 30.2 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 116 kB 59.3 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/e6/c1/4c6bcdf7a820034aa91a8b4d25fef38809be79b42ca7aaa16d4680b0bbac/pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40.0 MB 17.3 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting aiohttp\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/03/be/7ad9a6cd2312221cf7b6837d8e2d8e4660fbd4f9f15bccf79ef857f41f4d/aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 56.5 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: pandas in /root/miniconda3/lib/python3.8/site-packages (from datasets) (2.0.1)\n",
      "Collecting yarl<2.0,>=1.12.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c2/1e/1c78c695a4c7b957b5665e46a89ea35df48511dbed301a05c0a8beed0cc3/yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 319 kB 74.0 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/fe/ba/e2081de779ca30d473f21f5b30e0e737c438205440784c7dfc81efc2b029/async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/2e/a9/83692e37d8152f104333132105b67100aabfb2e96a87f6bed67f566035a7/multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129 kB 69.8 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/66/30/f9c006223feb2ac87f1826b57f2367b60aacc43092f562dab60d2312562e/frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 243 kB 73.9 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/b9/74/fbb6559de3607b3300b9be3cc64e97548d55678e44623db17820dbd20002/aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c3/f1/69a30ff0928d07f50bdc6f0147fd9a08e80904fd3fdb711785e518de1021/propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001B[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213 kB 78.4 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.8/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: propcache, multidict, frozenlist, yarl, tqdm, requests, hf-xet, fsspec, async-timeout, aiosignal, aiohappyeyeballs, huggingface-hub, dill, aiohttp, xxhash, tokenizers, safetensors, regex, pyarrow, multiprocess, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.10.11 aiosignal-1.3.1 async-timeout-5.0.1 datasets-3.1.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 hf-xet-1.2.0 huggingface-hub-0.36.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-17.0.0 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.46.3 xxhash-3.6.0 yarl-1.15.2\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5494a1-e4d3-4e63-9000-4795c6c581fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/commands/download.py:141: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "\u001B[33m‚ö†Ô∏è  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001B[0m\n",
      "/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 11 files:   0%|                                 | 0/11 [00:00<?, ?it/s]Downloading 'model.safetensors' to 'wav2vec2_local/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.8aa76ab2243c81747a1f832954586bc566090c83a0ac167df6f31f0fa917d74a.incomplete'\n",
      "Downloading 'pytorch_model.bin' to 'wav2vec2_local/.cache/huggingface/download/Q1p2l2BzM1m6P5jKvr8WTq1TUio=.c34f9827b034a1b9141dbf6f652f8a60eda61cdf5771c9e05bfa99033c92cd96.incomplete'\n",
      "Downloading 'feature_extractor_config.json' to 'wav2vec2_local/.cache/huggingface/download/4VpR2OeiJgUtBk492d08IL99QYQ=.52fdd74dc06f40033506e402269fbde5e7adc21d.incomplete'\n",
      "Downloading 'config.json' to 'wav2vec2_local/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.8ca9cc7496e145e37d09cec17d0c3bf9b8523c8e.incomplete'\n",
      "\n",
      "feature_extractor_config.json: 158B [00:00, 571kB/s]A\n",
      "Download complete. Moving file to wav2vec2_local/feature_extractor_config.json\n",
      "Downloading '.gitattributes' to 'wav2vec2_local/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.cf6d51fc9b1a671c35e92d6bd009880937aaa12d.incomplete'\n",
      "\n",
      "config.json: 1.60kB [00:00, 6.74MB/s]\n",
      "Download complete. Moving file to wav2vec2_local/config.json\n",
      "Downloading 'preprocessor_config.json' to 'wav2vec2_local/.cache/huggingface/download/PYH5dHjks7Ei0Yd3X0Z8xIwsCNQ=.3f24dc078fcba55ee1d417a413847ead40c093a3.incomplete'\n",
      "Downloading 'README.md' to 'wav2vec2_local/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.c7fe2047d7ac9b9816848c657b2a492ee95b264b.incomplete'\n",
      "\n",
      ".gitattributes: 790B [00:00, 114kB/s]A\n",
      "Download complete. Moving file to wav2vec2_local/.gitattributes\n",
      "Fetching 11 files:   9%|‚ñà‚ñà‚ñé                      | 1/11 [00:00<00:06,  1.55it/s]\n",
      "preprocessor_config.json: 159B [00:00, 767kB/s]A\n",
      "Download complete. Moving file to wav2vec2_local/preprocessor_config.json\n",
      "\n",
      "README.md: 4.43kB [00:00, 9.57MB/s]\n",
      "Download complete. Moving file to wav2vec2_local/README.md\n",
      "Downloading 'tf_model.h5' to 'wav2vec2_local/.cache/huggingface/download/a7eHxRFT3OeMBIFg52k2nfj5m7w=.412742825972a6e2e877255ccd8b3416e618df15a7f1e5e4f736aa3632ce33b5.incomplete'\n",
      "Downloading 'tokenizer_config.json' to 'wav2vec2_local/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.978a15a96dbb2d23e2afbc70137cae6c5ce38c8d.incomplete'\n",
      "\n",
      "tokenizer_config.json: 163B [00:00, 730kB/s]A\n",
      "Download complete. Moving file to wav2vec2_local/tokenizer_config.json\n",
      "Downloading 'vocab.json' to 'wav2vec2_local/.cache/huggingface/download/j3m-Hy6QvBddw8RXA1uSWl1AJ0c=.88181b954aa14df68be9b444b3c36585f3078c0a.incomplete'\n",
      "\n",
      "model.safetensors:   0%|                             | 0.00/378M [00:00<?, ?B/s]\u001B[A\n",
      "\n",
      "vocab.json: 291B [00:00, 817kB/s]A\u001B[A\n",
      "Download complete. Moving file to wav2vec2_local/vocab.json\n",
      "\n",
      "\n",
      "pytorch_model.bin:   0%|                             | 0.00/378M [00:00<?, ?B/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:   0%|                                   | 0.00/378M [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:   3%|‚ñå                   | 10.5M/378M [00:02<01:13, 4.99MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:   3%|‚ñå                   | 10.5M/378M [00:02<01:43, 3.55MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:   3%|‚ñã                         | 10.5M/378M [00:03<02:05, 2.92MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:   6%|‚ñà                   | 21.0M/378M [00:05<01:28, 4.03MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:   6%|‚ñà‚ñç                        | 21.0M/378M [00:05<01:20, 4.42MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:   6%|‚ñà                   | 21.0M/378M [00:06<01:44, 3.40MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:   8%|‚ñà‚ñà‚ñè                       | 31.5M/378M [00:07<01:14, 4.66MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:   8%|‚ñà‚ñã                  | 31.5M/378M [00:07<01:30, 3.82MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:   8%|‚ñà‚ñã                  | 31.5M/378M [00:08<01:35, 3.64MB/s]\u001B[A\u001B[A'(MaxRetryError(\"HTTPSConnectionPool(host='hf-mirror.com', port=443): Max retries exceeded with url: /facebook/wav2vec2-base-960h/resolve/22aad52d435eb6dbaf354bdad9b0da84ce7d6156/special_tokens_map.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f86f412f6d0>, 'Connection to hf-mirror.com timed out. (connect timeout=10)'))\"), '(Request ID: 045735b7-5b5a-4610-85f6-1bf176b8845a)')' thrown while requesting HEAD https://hf-mirror.com/facebook/wav2vec2-base-960h/resolve/22aad52d435eb6dbaf354bdad9b0da84ce7d6156/special_tokens_map.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "\n",
      "\n",
      "\n",
      "tf_model.h5:  11%|‚ñà‚ñà‚ñâ                       | 41.9M/378M [00:09<01:13, 4.57MB/s]\u001B[A\u001B[A\u001B[ADownloading 'special_tokens_map.json' to 'wav2vec2_local/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.25bc39604f72700b3b8e10bd69bb2f227157edd1.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "special_tokens_map.json: 85.0B [00:00, 189kB/s]\u001B[A\u001B[A\u001B[A\n",
      "Download complete. Moving file to wav2vec2_local/special_tokens_map.json\n",
      "\n",
      "model.safetensors:  11%|‚ñà‚ñà‚ñè                 | 41.9M/378M [00:10<01:30, 3.70MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  14%|‚ñà‚ñà‚ñà‚ñå                      | 52.4M/378M [00:11<01:05, 4.99MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  11%|‚ñà‚ñà‚ñè                 | 41.9M/378M [00:11<01:36, 3.48MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  17%|‚ñà‚ñà‚ñà‚ñà‚ñé                     | 62.9M/378M [00:13<01:06, 4.76MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  14%|‚ñà‚ñà‚ñä                 | 52.4M/378M [00:14<01:35, 3.41MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  14%|‚ñà‚ñà‚ñä                 | 52.4M/378M [00:15<01:41, 3.20MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà                     | 73.4M/378M [00:15<01:01, 4.96MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  17%|‚ñà‚ñà‚ñà‚ñé                | 62.9M/378M [00:17<01:28, 3.55MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 83.9M/378M [00:17<00:57, 5.10MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  17%|‚ñà‚ñà‚ñà‚ñé                | 62.9M/378M [00:18<01:35, 3.31MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  19%|‚ñà‚ñà‚ñà‚ñâ                | 73.4M/378M [00:19<01:22, 3.68MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 94.4M/378M [00:19<00:57, 4.90MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  22%|‚ñà‚ñà‚ñà‚ñà‚ñç               | 83.9M/378M [00:21<01:12, 4.08MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  19%|‚ñà‚ñà‚ñà‚ñâ                | 73.4M/378M [00:21<01:30, 3.36MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 105M/378M [00:22<00:59, 4.57MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  22%|‚ñà‚ñà‚ñà‚ñà‚ñç               | 83.9M/378M [00:24<01:24, 3.49MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 115M/378M [00:25<01:00, 4.37MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  25%|‚ñà‚ñà‚ñà‚ñà‚ñâ               | 94.4M/378M [00:25<01:21, 3.48MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  25%|‚ñà‚ñà‚ñà‚ñà‚ñâ               | 94.4M/378M [00:27<01:18, 3.62MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 126M/378M [00:27<00:53, 4.68MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 105M/378M [00:28<01:17, 3.51MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 105M/378M [00:29<01:09, 3.93MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 136M/378M [00:29<00:54, 4.44MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 115M/378M [00:31<01:12, 3.63MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 115M/378M [00:31<01:01, 4.23MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 147M/378M [00:32<00:55, 4.14MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 126M/378M [00:33<00:59, 4.20MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 126M/378M [00:34<01:09, 3.61MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 136M/378M [00:36<00:56, 4.28MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 157M/378M [00:35<00:57, 3.83MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 136M/378M [00:37<01:06, 3.65MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 147M/378M [00:38<00:53, 4.32MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 168M/378M [00:39<00:57, 3.67MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 147M/378M [00:39<01:00, 3.79MB/s]\u001B[A\n",
      "model.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 157M/378M [00:41<00:51, 4.25MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 157M/378M [00:41<00:54, 4.04MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 178M/378M [00:41<00:52, 3.79MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 168M/378M [00:43<00:49, 4.26MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 168M/378M [00:44<00:54, 3.88MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 189M/378M [00:44<00:49, 3.78MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 178M/378M [00:46<00:46, 4.27MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 199M/378M [00:46<00:45, 3.89MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 178M/378M [00:47<00:53, 3.72MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 189M/378M [00:49<00:46, 4.05MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 210M/378M [00:48<00:40, 4.18MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 189M/378M [00:50<00:51, 3.68MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 220M/378M [00:51<00:39, 4.04MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 199M/378M [00:52<00:46, 3.85MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 199M/378M [00:52<00:44, 4.03MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 231M/378M [00:53<00:33, 4.36MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 210M/378M [00:55<00:45, 3.71MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 210M/378M [00:55<00:43, 3.84MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 241M/378M [00:56<00:32, 4.21MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 220M/378M [00:57<00:39, 4.03MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 220M/378M [00:58<00:43, 3.64MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 252M/378M [00:58<00:29, 4.35MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 262M/378M [01:00<00:23, 4.92MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 231M/378M [01:01<00:39, 3.68MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 231M/378M [01:02<00:44, 3.33MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 273M/378M [01:01<00:19, 5.27MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 283M/378M [01:03<00:17, 5.47MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 241M/378M [01:04<00:36, 3.71MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 241M/378M [01:05<00:42, 3.19MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 294M/378M [01:05<00:15, 5.40MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 252M/378M [01:06<00:33, 3.73MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 304M/378M [01:07<00:14, 5.22MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 252M/378M [01:09<00:41, 3.04MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 262M/378M [01:09<00:31, 3.68MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315M/378M [01:09<00:12, 5.03MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 262M/378M [01:12<00:34, 3.30MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 325M/378M [01:12<00:10, 4.83MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 273M/378M [01:12<00:29, 3.59MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 273M/378M [01:14<00:28, 3.73MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 336M/378M [01:15<00:09, 4.48MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 283M/378M [01:16<00:27, 3.41MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 283M/378M [01:16<00:24, 3.90MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 346M/378M [01:17<00:06, 4.55MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 294M/378M [01:19<00:21, 3.94MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 294M/378M [01:19<00:24, 3.46MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 357M/378M [01:19<00:04, 4.65MB/s]\u001B[A\u001B[A\u001B[A\n",
      "model.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 304M/378M [01:21<00:18, 4.02MB/s]\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 367M/378M [01:21<00:02, 4.60MB/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 304M/378M [01:22<00:22, 3.32MB/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "tf_model.h5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 378M/378M [01:23<00:00, 4.51MB/s]\u001B[A\u001B[A\u001B[A\n",
      "Download complete. Moving file to wav2vec2_local/tf_model.h5\n",
      "\n",
      "model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 315M/378M [01:24<00:17, 3.69MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 315M/378M [01:26<00:20, 3.07MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 325M/378M [01:27<00:13, 3.96MB/s]\u001B[A\n",
      "model.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 336M/378M [01:29<00:09, 4.21MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 325M/378M [01:29<00:16, 3.16MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 346M/378M [01:30<00:06, 5.05MB/s]\u001B[A\n",
      "model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 357M/378M [01:31<00:03, 6.22MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 336M/378M [01:32<00:12, 3.50MB/s]\u001B[A\u001B[A\n",
      "model.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 367M/378M [01:32<00:01, 6.90MB/s]\u001B[A\n",
      "\n",
      "pytorch_model.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 346M/378M [01:33<00:07, 3.99MB/s]\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 357M/378M [01:35<00:04, 4.52MB/s]\u001B[A\u001B[A\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 378M/378M [01:36<00:00, 3.93MB/s]\u001B[A\n",
      "Download complete. Moving file to wav2vec2_local/model.safetensors\n",
      "Fetching 11 files:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 5/11 [01:37<02:03, 20.63s/it]\n",
      "\n",
      "pytorch_model.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 367M/378M [01:36<00:01, 5.54MB/s]\u001B[A\u001B[A\n",
      "\n",
      "pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 378M/378M [01:37<00:00, 3.89MB/s]\u001B[A\u001B[A\n",
      "Download complete. Moving file to wav2vec2_local/pytorch_model.bin\n",
      "Fetching 11 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [01:38<00:00,  8.94s/it]\n",
      "/root/VED/wav2vec2_local\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# üöÄ ÁªàÊûÅ‰∏ãËΩΩÊñπÊ°àÔºö‰ΩøÁî®ÂëΩ‰ª§Ë°å + Âº∫Âà∂ÈïúÂÉè\n",
    "# =======================================================\n",
    "# Ëøô‰∏ÄÊ≠•‰ºöÂ∞ÜÊ®°Âûã‰∏ãËΩΩÂà∞ÂΩìÂâçÁõÆÂΩï‰∏ãÁöÑ \"wav2vec2_local\" Êñá‰ª∂Â§π‰∏≠\n",
    "# Âç≥‰ΩøÊ≤°ÊúâÊ¢ØÂ≠êÔºåÂú®ÂõΩÂÜÖÊúçÂä°Âô®ÈÄöÂ∏∏‰πüËÉΩË∑ëÊª°ÂÆΩÂ∏¶\n",
    "# =======================================================\n",
    "\n",
    "!export HF_ENDPOINT=https://hf-mirror.com && huggingface-cli download \\\n",
    "    --resume-download facebook/wav2vec2-base-960h \\\n",
    "    --local-dir ./wav2vec2_local \\\n",
    "    --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7da22d9-f12c-4682-b030-10551f3275c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï ===\n",
      "/root/VED\n",
      "\n",
      "=== ÂΩìÂâçÁõÆÂΩï‰∏ãÁöÑÊñá‰ª∂Â§π ===\n",
      "['AudioWAV', '.ipynb_checkpoints', 'voiceEmotionDetective.ipynb', 'best_cnn.h5', 'best_crnn.h5', 'best_transformer.h5', 'voiceEmotionDetective-Copy1.ipynb', 'VED-CNN-Transfomer.ipynb', 'test.ipynb', 'best_cnn_transformer_ser.h5', 'best_conformer.h5', 'confusion_matrix_conformer.png', 'training_curves_conformer.png', 'class_f1_conformer.png', 'best_conformer_v2.h5', 'training_log_v2.csv', 'ConformerV1.ipynb', 'ConformerV3.1.ipynb', 'Output', 'DoubelStreamV4.ipynb', 'DoubelStreamV4.1.ipynb', 'ConformerV3.2.ipynb', 'ConformerV3.ipynb', 'ConformerV2.ipynb', 'ConformerV3.3.ipynb', 'wav2vec2', 'wav2vec2_local', 'v1.ipynb']\n",
      "\n",
      "=== ÁõÆÊ†áÊñá‰ª∂Â§π wav2vec2_local ÈáåÁöÑÂÜÖÂÆπ ===\n",
      "['.cache', 'feature_extractor_config.json', 'config.json', '.gitattributes', 'preprocessor_config.json', 'README.md', 'tokenizer_config.json', 'vocab.json', 'special_tokens_map.json', 'tf_model.h5', 'model.safetensors', 'pytorch_model.bin']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"=== ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï ===\")\n",
    "print(os.getcwd())\n",
    "\n",
    "print(\"\\n=== ÂΩìÂâçÁõÆÂΩï‰∏ãÁöÑÊñá‰ª∂Â§π ===\")\n",
    "print(os.listdir(\"..\"))\n",
    "\n",
    "# Ê£ÄÊü•‰Ω†ËÆæÁΩÆÁöÑË∑ØÂæÑÊòØÂê¶Â≠òÂú®\n",
    "target_path = \"../wav2vec2_local\"  # ËøôÊòØ‰Ω†Êä•Èîô‰ø°ÊÅØÈáåÊòæÁ§∫ÁöÑË∑ØÂæÑ\n",
    "if os.path.exists(target_path):\n",
    "    print(f\"\\n=== ÁõÆÊ†áÊñá‰ª∂Â§π {target_path} ÈáåÁöÑÂÜÖÂÆπ ===\")\n",
    "    print(os.listdir(target_path))\n",
    "else:\n",
    "    print(f\"\\n‚ùå ÈîôËØØÔºöÊñá‰ª∂Â§π '{target_path}' Ê†πÊú¨‰∏çÂ≠òÂú®ÔºÅ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f52344-2752-4b1f-b56e-c5196da04c03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 09:14:13.628389: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 09:14:13.691056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-04 09:14:14.706477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 7442\n",
      "Train: 5890, Test: 1552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 09:14:18.845795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31132 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:65:03.0, compute capability: 7.0\n",
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2025-12-04 09:14:20.187673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
      "2025-12-04 09:14:20.600394: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c94782be90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-04 09:14:20.600428: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2025-12-04 09:14:20.949352: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFWav2Vec2Model were not initialized from the PyTorch model and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " audio_input (InputLayer)    [(None, 48000)]           0         \n",
      "                                                                 \n",
      " tf_wav2_vec2_model (TFWav2  TFWav2Vec2BaseModelOutp   94371712  \n",
      " Vec2Model)                  ut(last_hidden_state=(N             \n",
      "                             one, 149, 768),                     \n",
      "                              extract_features=(None             \n",
      "                             , 149, 512),                        \n",
      "                              hidden_states=None, at             \n",
      "                             tentions=None)                      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 768)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " emotion (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94602246 (360.88 MB)\n",
      "Trainable params: 230534 (900.52 KB)\n",
      "Non-trainable params: 94371712 (360.00 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 09:14:49.092644: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - ETA: 0s - loss: 1.7484 - accuracy: 0.2329\n",
      "Epoch 1: val_accuracy improved from -inf to 0.20232, saving model to Output/wav2vec2_20251204_091418/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 71s 283ms/step - loss: 1.7484 - accuracy: 0.2329 - val_loss: 1.9231 - val_accuracy: 0.2023\n",
      "Epoch 2/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6848 - accuracy: 0.2854\n",
      "Epoch 2: val_accuracy did not improve from 0.20232\n",
      "185/185 [==============================] - 47s 251ms/step - loss: 1.6848 - accuracy: 0.2854 - val_loss: 2.0174 - val_accuracy: 0.1907\n",
      "Epoch 3/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6602 - accuracy: 0.3024\n",
      "Epoch 3: val_accuracy did not improve from 0.20232\n",
      "185/185 [==============================] - 46s 250ms/step - loss: 1.6602 - accuracy: 0.3024 - val_loss: 1.9712 - val_accuracy: 0.2017\n",
      "Epoch 4/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6561 - accuracy: 0.3041\n",
      "Epoch 4: val_accuracy improved from 0.20232 to 0.24678, saving model to Output/wav2vec2_20251204_091418/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 49s 262ms/step - loss: 1.6561 - accuracy: 0.3041 - val_loss: 1.8418 - val_accuracy: 0.2468\n",
      "Epoch 5/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6481 - accuracy: 0.3024\n",
      "Epoch 5: val_accuracy did not improve from 0.24678\n",
      "185/185 [==============================] - 47s 251ms/step - loss: 1.6481 - accuracy: 0.3024 - val_loss: 2.2767 - val_accuracy: 0.1836\n",
      "Epoch 6/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6342 - accuracy: 0.3153\n",
      "Epoch 6: val_accuracy improved from 0.24678 to 0.26869, saving model to Output/wav2vec2_20251204_091418/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 48s 260ms/step - loss: 1.6342 - accuracy: 0.3153 - val_loss: 1.7607 - val_accuracy: 0.2687\n",
      "Epoch 7/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6279 - accuracy: 0.3122\n",
      "Epoch 7: val_accuracy did not improve from 0.26869\n",
      "185/185 [==============================] - 46s 250ms/step - loss: 1.6279 - accuracy: 0.3122 - val_loss: 1.9797 - val_accuracy: 0.2133\n",
      "Epoch 8/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6370 - accuracy: 0.3112\n",
      "Epoch 8: val_accuracy did not improve from 0.26869\n",
      "185/185 [==============================] - 46s 249ms/step - loss: 1.6370 - accuracy: 0.3112 - val_loss: 1.9618 - val_accuracy: 0.1894\n",
      "Epoch 9/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6304 - accuracy: 0.3126\n",
      "Epoch 9: val_accuracy did not improve from 0.26869\n",
      "185/185 [==============================] - 46s 248ms/step - loss: 1.6304 - accuracy: 0.3126 - val_loss: 1.9785 - val_accuracy: 0.2410\n",
      "Epoch 10/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6209 - accuracy: 0.3139\n",
      "Epoch 10: val_accuracy did not improve from 0.26869\n",
      "185/185 [==============================] - 47s 252ms/step - loss: 1.6209 - accuracy: 0.3139 - val_loss: 1.9794 - val_accuracy: 0.2442\n",
      "Epoch 11/30\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.6254 - accuracy: 0.3146\n",
      "Epoch 11: val_accuracy did not improve from 0.26869\n",
      "185/185 [==============================] - 46s 250ms/step - loss: 1.6254 - accuracy: 0.3146 - val_loss: 1.9304 - val_accuracy: 0.2294\n",
      "Evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG     0.2549    0.7925    0.3857       265\n",
      "         DIS     0.4766    0.1925    0.2742       265\n",
      "         FEA     0.2727    0.0453    0.0777       265\n",
      "         HAP     0.2500    0.0566    0.0923       265\n",
      "         NEU     0.2309    0.4670    0.3090       227\n",
      "         SAD     0.3966    0.0868    0.1424       265\n",
      "\n",
      "    accuracy                         0.2687      1552\n",
      "   macro avg     0.3136    0.2734    0.2135      1552\n",
      "weighted avg     0.3156    0.2687    0.2112      1552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SER using Pre-trained Wav2Vec 2.0\n",
    "# Á≠ñÁï•: ‰ΩøÁî® Wav2Vec 2.0 ‰Ωú‰∏∫ÁâπÂæÅÊèêÂèñÂô® (Feature Extractor)\n",
    "# ËæìÂÖ•: Raw Audio (16kHz) -> Wav2Vec2 -> Dense Head -> Emotion\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import TFWav2Vec2Model, Wav2Vec2Processor\n",
    "\n",
    "# ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# =====================\n",
    "# 1. ÈÖçÁΩÆ\n",
    "# =====================\n",
    "# ËøôÈáåÁöÑÊ®°ÂûãÂèØ‰ª•Êç¢Êàê \"microsoft/wavlm-base-plus\" (ÊïàÊûúÊõ¥Â•ΩÔºå‰ΩÜÈúÄË¶ÅÂÖºÂÆπÊÄßÊ£ÄÊü•)\n",
    "MODEL_CHECKPOINT = \"wav2vec2_local\" \n",
    "AUDIO_DIR = Path(\"../AudioWAV\")\n",
    "BATCH_SIZE = 32 # È¢ÑËÆ≠ÁªÉÊ®°ÂûãÊòæÂ≠òÂç†Áî®Â§ßÔºåÂª∫ËÆÆ‰ªé 8 Êàñ 16 ÂºÄÂßã\n",
    "MAX_DURATION = 3.0 # Áßí\n",
    "SR = 16000 # ÂøÖÈ°ªÊòØ 16kÔºåËøôÊòØÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑËßÑÂÆö\n",
    "\n",
    "# Ëá™Âä®ÂàõÂª∫ÁõÆÂΩï\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = f\"Output/wav2vec2_{timestamp}\"\n",
    "os.makedirs(f\"{EXP_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# =====================\n",
    "# 2. Êï∞ÊçÆÂáÜÂ§á (Raw Audio)\n",
    "# =====================\n",
    "def build_metadata(audio_dir):\n",
    "    records = []\n",
    "    for wav in audio_dir.glob(\"*.wav\"):\n",
    "        parts = wav.stem.split(\"_\")\n",
    "        if len(parts) == 4:\n",
    "            records.append({\"path\": str(wav), \"spk\": parts[0], \"emo\": parts[2]})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "meta = build_metadata(AUDIO_DIR)\n",
    "print(f\"Samples: {len(meta)}\")\n",
    "\n",
    "# ÂàíÂàÜ\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(meta['emo'])\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "tr_idx, te_idx = next(gss.split(meta, y, groups=meta['spk']))\n",
    "train_df = meta.iloc[tr_idx]\n",
    "test_df = meta.iloc[te_idx]\n",
    "y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# =====================\n",
    "# 3. Êï∞ÊçÆÁîüÊàêÂô® (Âä®ÊÄÅÂä†ËΩΩ)\n",
    "# =====================\n",
    "# È¢ÑËÆ≠ÁªÉÊ®°ÂûãËæìÂÖ•ÊòØÂéüÂßãÊ≥¢ÂΩ¢Ôºå‰∏çËÉΩÂÉè Log-Mel ‰∏ÄÊ†∑ÂÖ®ÈÉ®ËØªÂÖ•ÂÜÖÂ≠òÔºàÂ§™Âç†Á©∫Èó¥Ôºâ\n",
    "# Êàë‰ª¨‰ΩøÁî® Hugging Face ÁöÑ Processor Â§ÑÁêÜÊï∞ÊçÆ\n",
    "\n",
    "# processor = Wav2Vec2Processor.from_pretrained(MODEL_CHECKPOINT)\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_CHECKPOINT, local_files_only=True)\n",
    "TARGET_LEN = int(SR * MAX_DURATION)\n",
    "\n",
    "class AudioGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, y, batch_size=16, shuffle=True, augment=False):\n",
    "        self.df = df\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.indexes = np.arange(len(df))\n",
    "        if shuffle: np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = self.indexes[idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "        batch_paths = self.df.iloc[batch_idx]['path'].values\n",
    "        batch_y = self.y[batch_idx]\n",
    "\n",
    "        batch_x = []\n",
    "        for p in batch_paths:\n",
    "            # ËØªÂèñÈü≥È¢ë\n",
    "            speech, _ = librosa.load(p, sr=SR)\n",
    "            \n",
    "            # ÁÆÄÂçïÊï∞ÊçÆÂ¢ûÂº∫ (ÂèØÈÄâ)ÔºöÈöèÊú∫Ë£ÅÂâ™ÊàñÂ°´ÂÖÖ\n",
    "            if self.augment and len(speech) > TARGET_LEN:\n",
    "                start = np.random.randint(0, len(speech) - TARGET_LEN)\n",
    "                speech = speech[start : start+TARGET_LEN]\n",
    "            else:\n",
    "                # Â±Ö‰∏≠Êà™Êñ≠/Â°´ÂÖÖ\n",
    "                if len(speech) > TARGET_LEN:\n",
    "                    speech = speech[:TARGET_LEN]\n",
    "                else:\n",
    "                    pad_width = TARGET_LEN - len(speech)\n",
    "                    speech = np.pad(speech, (0, pad_width))\n",
    "            \n",
    "            batch_x.append(speech)\n",
    "\n",
    "        # ‰ΩøÁî® Processor ËøõË°åÊ†áÂáÜÂåñ (ËøîÂõû tensor)\n",
    "        # HuggingFace processor Ë¥üË¥£ÂΩí‰∏ÄÂåñ\n",
    "        inputs = processor(batch_x, sampling_rate=SR, return_tensors=\"tf\", padding=True)\n",
    "        \n",
    "        # Â∞ÜÊ†áÁ≠æËΩ¨‰∏∫ one-hot\n",
    "        batch_y_oh = tf.keras.utils.to_categorical(batch_y, num_classes=len(le.classes_))\n",
    "        \n",
    "        return inputs.input_values, batch_y_oh\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: np.random.shuffle(self.indexes)\n",
    "\n",
    "train_gen = AudioGenerator(train_df, y_tr, BATCH_SIZE, augment=True)\n",
    "val_gen = AudioGenerator(test_df, y_te, BATCH_SIZE, shuffle=False, augment=False)\n",
    "\n",
    "# =====================\n",
    "# 4. ÊûÑÂª∫Ê®°Âûã\n",
    "# =====================\n",
    "def build_wav2vec_model(num_classes):\n",
    "    # 1. Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉ‰∏ª‰Ωì\n",
    "    # from_pt=True Ë°®Á§∫Â¶ÇÊûúÂè™Êúâ PyTorch ÊùÉÈáçÔºåËá™Âä®ËΩ¨Êç¢ÔºàÁ¨¨‰∏ÄÊ¨°ËøêË°åÂèØËÉΩÈúÄË¶ÅÔºâ\n",
    "    # backbone = TFWav2Vec2Model.from_pretrained(MODEL_CHECKPOINT, from_pt=True)\n",
    "    backbone = TFWav2Vec2Model.from_pretrained(MODEL_CHECKPOINT, from_pt=True, local_files_only=True)\n",
    "    \n",
    "    # 2. ÂÜªÁªì‰∏ª‰Ωì (Feature Extraction Ê®°Âºè)\n",
    "    # Â¶ÇÊûúÊÉ≥Ë¶Å Fine-tuningÔºåÂ∞ÜËøôÈáåËÆæ‰∏∫ TrueÔºå‰ΩÜÈúÄË¶ÅÊûÅÂ∞èÁöÑ LR\n",
    "    backbone.trainable = False \n",
    "    \n",
    "    # 3. ÂÆö‰πâËæìÂÖ•\n",
    "    # Wav2Vec2 ËæìÂÖ•ÂΩ¢Áä∂: (Batch, Time)\n",
    "    input_audio = layers.Input(shape=(TARGET_LEN,), dtype=tf.float32, name=\"audio_input\")\n",
    "    \n",
    "    # 4. ÂâçÂêë‰º†Êí≠\n",
    "    # backbone ËøîÂõû‰∏Ä‰∏™ÂØπË±°Ôºålast_hidden_state ÊòØ (Batch, Sequence, Hidden_Dim)\n",
    "    # e.g., (Batch, 149, 768)\n",
    "    outputs = backbone(input_audio)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    # 5. ‰∏ãÊ∏∏ÂàÜÁ±ªÂ§¥ (Downstream Head)\n",
    "    # ÊñπÊ≥ï A: Mean Pooling (ÂØπÊó∂Èó¥Áª¥Â∫¶Ê±ÇÂπ≥Âùá)\n",
    "    x = layers.GlobalAveragePooling1D()(hidden_states)\n",
    "    \n",
    "    # ÊñπÊ≥ï B: ‰πüÂèØ‰ª•Êé• LSTM Êàñ ConformerÔºå‰ΩÜÈÄöÂ∏∏ MLP Â∞±Â§ü‰∫Ü\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output\n",
    "    prediction = layers.Dense(num_classes, activation=\"softmax\", name=\"emotion\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=input_audio, outputs=prediction)\n",
    "    return model\n",
    "\n",
    "# =====================\n",
    "# 5. ËÆ≠ÁªÉ\n",
    "# =====================\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_wav2vec_model(len(le.classes_))\n",
    "model.summary()\n",
    "\n",
    "# ‰ºòÂåñÂô®ÔºöÂõ†‰∏∫Âè™ËÆ≠ÁªÉÂàÜÁ±ªÂ§¥ÔºåLR ÂèØ‰ª•Á®çÂ§ß (1e-3 or 1e-4)\n",
    "# Â¶ÇÊûú backbone.trainable = TrueÔºåLR ÂøÖÈ°ªÊòØ 1e-5\n",
    "optimizer = optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ÂõûË∞É\n",
    "class MacroF1Callback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # ‰∏∫‰∫ÜÈÄüÂ∫¶ÔºåÊØè 3 ‰∏™ epoch Êµã‰∏ÄÊ¨°ÔºåÊàñËÄÖÂú®È™åËØÅÈõÜËæÉÂ§ßÊó∂ÊäΩÊ†∑\n",
    "        pass \n",
    "        # Ê≥®ÊÑèÔºöÁî±‰∫é Generator ÊØèÊ¨°ÁîüÊàê‰∏ÄÊâπÔºåÂÖ®ÈáèÈ¢ÑÊµãÂèØËÉΩËÄóÊó∂ÔºåËøôÈáåÊöÇÊó∂‰æùËµñ val_accuracy\n",
    "\n",
    "ckpt = callbacks.ModelCheckpoint(\n",
    "    f\"{EXP_DIR}/models/best_wav2vec.h5\", \n",
    "    monitor=\"val_accuracy\", \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, # HuggingFace Ê®°ÂûãÂª∫ËÆÆÂè™Â≠òÊùÉÈáç\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=30, # È¢ÑËÆ≠ÁªÉÊ®°ÂûãÊî∂ÊïõÈÄöÂ∏∏ÂæàÂø´\n",
    "    callbacks=[ckpt, callbacks.EarlyStopping(patience=5, monitor=\"val_accuracy\")],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# 6. ËØÑ‰º∞\n",
    "# =====================\n",
    "print(\"Evaluating...\")\n",
    "# ÈáçÊñ∞Âä†ËΩΩÊúÄ‰Ω≥ÊùÉÈáç\n",
    "model.load_weights(f\"{EXP_DIR}/models/best_wav2vec.h5\")\n",
    "\n",
    "# È¢ÑÊµã\n",
    "y_pred_indices = []\n",
    "y_true_indices = []\n",
    "\n",
    "for i in range(len(val_gen)):\n",
    "    x_batch, y_batch = val_gen[i]\n",
    "    preds = model.predict(x_batch, verbose=0)\n",
    "    y_pred_indices.extend(np.argmax(preds, axis=1))\n",
    "    y_true_indices.extend(np.argmax(y_batch, axis=1))\n",
    "\n",
    "print(classification_report(y_true_indices, y_pred_indices, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c368b5e-faad-4b8c-ae66-fea3124c8db9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 09:33:55.210264: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 09:33:55.277994: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-04 09:33:56.224365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GPU detected: 1\n",
      "Samples: 7442\n",
      "Train: 5890, Test: 1552\n",
      "‚úì Processor loaded from local path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 09:34:00.212756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31132 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:65:03.0, compute capability: 7.0\n",
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2025-12-04 09:34:01.113746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
      "2025-12-04 09:34:01.529392: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e99dccfc80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-04 09:34:01.529427: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2025-12-04 09:34:01.882094: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFWav2Vec2Model were not initialized from the PyTorch model and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded from local path.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " audio_input (InputLayer)    [(None, 48000)]              0         []                            \n",
      "                                                                                                  \n",
      " tf_wav2_vec2_model (TFWav2  TFWav2Vec2BaseModelOutput(   9437171   ['audio_input[0][0]']         \n",
      " Vec2Model)                  last_hidden_state=(None, 1   2                                       \n",
      "                             49, 768),                                                            \n",
      "                              extract_features=(None, 1                                           \n",
      "                             49, 512),                                                            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 149, 1)               769       ['tf_wav2_vec2_model[0][1]']  \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 149)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 149)                  0         ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 149, 1)               0         ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " multiply (Multiply)         (None, 149, 768)             0         ['tf_wav2_vec2_model[0][1]',  \n",
      "                                                                     'reshape[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLa  (None, 768)                  0         ['multiply[0][0]']            \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 256)                  196864    ['tf.math.reduce_sum[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)        (None, 256)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 128)                  32896     ['dropout_50[0][0]']          \n",
      "                                                                                                  \n",
      " emotion (Dense)             (None, 6)                    774       ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 94603015 (360.88 MB)\n",
      "Trainable params: 94603015 (360.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Starting Fine-Tuning (This will be slower)...\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.8/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 09:34:43.726345: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - ETA: 0s - loss: 1.6784 - accuracy: 0.2900\n",
      "Epoch 1: val_accuracy improved from -inf to 0.29188, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 173s 644ms/step - loss: 1.6784 - accuracy: 0.2900 - val_loss: 1.6633 - val_accuracy: 0.2919 - lr: 1.0000e-05\n",
      "Epoch 2/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.4547 - accuracy: 0.4190\n",
      "Epoch 2: val_accuracy improved from 0.29188 to 0.36534, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 113s 611ms/step - loss: 1.4547 - accuracy: 0.4190 - val_loss: 1.6049 - val_accuracy: 0.3653 - lr: 1.0000e-05\n",
      "Epoch 3/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.2893 - accuracy: 0.5163\n",
      "Epoch 3: val_accuracy improved from 0.36534 to 0.37693, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 113s 612ms/step - loss: 1.2893 - accuracy: 0.5163 - val_loss: 1.7350 - val_accuracy: 0.3769 - lr: 1.0000e-05\n",
      "Epoch 4/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 1.0970 - accuracy: 0.6066\n",
      "Epoch 4: val_accuracy improved from 0.37693 to 0.42332, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "185/185 [==============================] - 113s 611ms/step - loss: 1.0970 - accuracy: 0.6066 - val_loss: 1.7379 - val_accuracy: 0.4233 - lr: 1.0000e-05\n",
      "Epoch 5/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.8962 - accuracy: 0.6869\n",
      "Epoch 5: val_accuracy improved from 0.42332 to 0.49485, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 113s 612ms/step - loss: 0.8962 - accuracy: 0.6869 - val_loss: 1.4500 - val_accuracy: 0.4948 - lr: 5.0000e-06\n",
      "Epoch 6/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.8061 - accuracy: 0.7217\n",
      "Epoch 6: val_accuracy did not improve from 0.49485\n",
      "185/185 [==============================] - 112s 603ms/step - loss: 0.8061 - accuracy: 0.7217 - val_loss: 1.4834 - val_accuracy: 0.4865 - lr: 5.0000e-06\n",
      "Epoch 7/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.7443\n",
      "Epoch 7: val_accuracy did not improve from 0.49485\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "185/185 [==============================] - 111s 601ms/step - loss: 0.7436 - accuracy: 0.7443 - val_loss: 1.5382 - val_accuracy: 0.4910 - lr: 5.0000e-06\n",
      "Epoch 8/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.6747 - accuracy: 0.7662\n",
      "Epoch 8: val_accuracy did not improve from 0.49485\n",
      "185/185 [==============================] - 111s 601ms/step - loss: 0.6747 - accuracy: 0.7662 - val_loss: 1.5028 - val_accuracy: 0.4916 - lr: 2.5000e-06\n",
      "Epoch 9/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.6518 - accuracy: 0.7779\n",
      "Epoch 9: val_accuracy improved from 0.49485 to 0.50838, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 113s 612ms/step - loss: 0.6518 - accuracy: 0.7779 - val_loss: 1.4367 - val_accuracy: 0.5084 - lr: 2.5000e-06\n",
      "Epoch 10/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.7868\n",
      "Epoch 10: val_accuracy improved from 0.50838 to 0.51611, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 113s 613ms/step - loss: 0.6281 - accuracy: 0.7868 - val_loss: 1.4380 - val_accuracy: 0.5161 - lr: 2.5000e-06\n",
      "Epoch 11/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.6164 - accuracy: 0.7902\n",
      "Epoch 11: val_accuracy improved from 0.51611 to 0.51933, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 113s 611ms/step - loss: 0.6164 - accuracy: 0.7902 - val_loss: 1.4363 - val_accuracy: 0.5193 - lr: 2.5000e-06\n",
      "Epoch 12/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.5841 - accuracy: 0.7997\n",
      "Epoch 12: val_accuracy improved from 0.51933 to 0.52191, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 113s 612ms/step - loss: 0.5841 - accuracy: 0.7997 - val_loss: 1.4274 - val_accuracy: 0.5219 - lr: 2.5000e-06\n",
      "Epoch 13/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.8107\n",
      "Epoch 13: val_accuracy improved from 0.52191 to 0.52577, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "185/185 [==============================] - 113s 612ms/step - loss: 0.5603 - accuracy: 0.8107 - val_loss: 1.4131 - val_accuracy: 0.5258 - lr: 2.5000e-06\n",
      "Epoch 14/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.5433 - accuracy: 0.8158\n",
      "Epoch 14: val_accuracy did not improve from 0.52577\n",
      "185/185 [==============================] - 111s 602ms/step - loss: 0.5433 - accuracy: 0.8158 - val_loss: 1.4733 - val_accuracy: 0.5219 - lr: 2.5000e-06\n",
      "Epoch 15/15\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.8160\n",
      "Epoch 15: val_accuracy improved from 0.52577 to 0.53286, saving model to Output/wav2vec2_finetune_20251204_093400/models/best_wav2vec.h5\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "185/185 [==============================] - 113s 611ms/step - loss: 0.5356 - accuracy: 0.8160 - val_loss: 1.4148 - val_accuracy: 0.5329 - lr: 2.5000e-06\n",
      "Evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG     0.5000    0.9208    0.6481       265\n",
      "         DIS     0.6774    0.2377    0.3520       265\n",
      "         FEA     0.5619    0.4113    0.4749       265\n",
      "         HAP     0.4739    0.4453    0.4591       265\n",
      "         NEU     0.7655    0.4890    0.5968       227\n",
      "         SAD     0.4752    0.6868    0.5617       265\n",
      "\n",
      "    accuracy                         0.5329      1552\n",
      "   macro avg     0.5756    0.5318    0.5154      1552\n",
      "weighted avg     0.5710    0.5329    0.5134      1552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SER using Pre-trained Wav2Vec 2.0 (Fine-Tuning Version)\n",
    "# Á≠ñÁï•: Ëß£ÂÜª Wav2Vec 2.0 ËøõË°åÂæÆË∞É (Fine-Tuning)\n",
    "# ÈíàÂØπÁéØÂ¢É: Á¶ªÁ∫øÊ®°Âºè (Local Files) + ÊòæÂ≠ò‰ºòÂåñ\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "# Â±èËîΩ HF ‰∏ãËΩΩË≠¶Âëä\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import TFWav2Vec2Model, Wav2Vec2Processor\n",
    "\n",
    "# ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ÊòæÂ≠òÊåâÈúÄÂàÜÈÖç (Èò≤Ê≠¢ OOM)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úì GPU detected: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# =====================\n",
    "# 1. ÈÖçÁΩÆ\n",
    "# =====================\n",
    "# ËØ∑Á°Æ‰øù MODEL_CHECKPOINT ÊåáÂêë‰Ω†Êú¨Âú∞Ëß£ÂéãÂ•ΩÁöÑÊ®°ÂûãÊñá‰ª∂Â§π\n",
    "MODEL_CHECKPOINT = \"wav2vec2_local\" \n",
    "AUDIO_DIR = Path(\"../AudioWAV\")\n",
    "\n",
    "# ÂæÆË∞ÉÊó∂ÁöÑ Batch Size ÂøÖÈ°ªÂæàÂ∞èÔºÅ\n",
    "BATCH_SIZE = 32  # Â¶ÇÊûúÊòæÂ≠òËøò‰∏çÂ§üÔºåÊîπÊàê 4\n",
    "MAX_DURATION = 3.0 # Áßí\n",
    "SR = 16000 \n",
    "\n",
    "# Ëá™Âä®ÂàõÂª∫ÁõÆÂΩï\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = f\"Output/wav2vec2_finetune_{timestamp}\"\n",
    "os.makedirs(f\"{EXP_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# =====================\n",
    "# 2. Êï∞ÊçÆÂáÜÂ§á (Raw Audio)\n",
    "# =====================\n",
    "def build_metadata(audio_dir):\n",
    "    records = []\n",
    "    if not audio_dir.exists():\n",
    "        print(f\"Error: Directory {audio_dir} does not exist.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    for wav in audio_dir.glob(\"*.wav\"):\n",
    "        parts = wav.stem.split(\"_\")\n",
    "        if len(parts) == 4:\n",
    "            records.append({\"path\": str(wav), \"spk\": parts[0], \"emo\": parts[2]})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "meta = build_metadata(AUDIO_DIR)\n",
    "print(f\"Samples: {len(meta)}\")\n",
    "\n",
    "if len(meta) == 0:\n",
    "    raise ValueError(\"No audio files found. Check AUDIO_DIR path.\")\n",
    "\n",
    "# ÂàíÂàÜ\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(meta['emo'])\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "tr_idx, te_idx = next(gss.split(meta, y, groups=meta['spk']))\n",
    "train_df = meta.iloc[tr_idx]\n",
    "test_df = meta.iloc[te_idx]\n",
    "y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# =====================\n",
    "# 3. Êï∞ÊçÆÁîüÊàêÂô®\n",
    "# =====================\n",
    "# Âä†ËΩΩÊú¨Âú∞ Processor\n",
    "try:\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_CHECKPOINT, local_files_only=True)\n",
    "    print(\"‚úì Processor loaded from local path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processor: {e}\")\n",
    "    # Â¶ÇÊûúÊú¨Âú∞Âä†ËΩΩÂ§±Ë¥•ÔºåÂ∞ùËØïÂú®Á∫øÂä†ËΩΩÔºàÂ¶ÇÊûúÁΩëÁªúÂÖÅËÆ∏ÔºâÊàñËÄÖÊä•ÈîôÂÅúÊ≠¢\n",
    "    raise e\n",
    "\n",
    "TARGET_LEN = int(SR * MAX_DURATION)\n",
    "\n",
    "class AudioGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, y, batch_size=8, shuffle=True, augment=False):\n",
    "        self.df = df\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.indexes = np.arange(len(df))\n",
    "        if shuffle: np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = self.indexes[idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "        batch_paths = self.df.iloc[batch_idx]['path'].values\n",
    "        batch_y = self.y[batch_idx]\n",
    "\n",
    "        batch_x = []\n",
    "        for p in batch_paths:\n",
    "            # ËØªÂèñÈü≥È¢ë\n",
    "            speech, _ = librosa.load(p, sr=SR)\n",
    "            \n",
    "            # Êï∞ÊçÆÂ¢ûÂº∫ÔºöÁÆÄÂçïÁöÑÈöèÊú∫Ë£ÅÂâ™/Âπ≥Áßª\n",
    "            if self.augment and len(speech) > TARGET_LEN:\n",
    "                offset = np.random.randint(0, len(speech) - TARGET_LEN)\n",
    "                speech = speech[offset : offset+TARGET_LEN]\n",
    "            else:\n",
    "                # Â±Ö‰∏≠Êà™Êñ≠/Â°´ÂÖÖ\n",
    "                if len(speech) > TARGET_LEN:\n",
    "                    speech = speech[:TARGET_LEN]\n",
    "                else:\n",
    "                    pad_width = TARGET_LEN - len(speech)\n",
    "                    speech = np.pad(speech, (0, pad_width))\n",
    "            \n",
    "            batch_x.append(speech)\n",
    "\n",
    "        # ‰ΩøÁî® Processor ËøõË°åÂΩí‰∏ÄÂåñ\n",
    "        # return_tensors=\"tf\" ËøîÂõû TF Tensor\n",
    "        inputs = processor(batch_x, sampling_rate=SR, return_tensors=\"tf\", padding=True)\n",
    "        \n",
    "        # Ê†áÁ≠æ one-hot\n",
    "        batch_y_oh = tf.keras.utils.to_categorical(batch_y, num_classes=len(le.classes_))\n",
    "        \n",
    "        return inputs.input_values, batch_y_oh\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: np.random.shuffle(self.indexes)\n",
    "\n",
    "train_gen = AudioGenerator(train_df, y_tr, BATCH_SIZE, augment=True)\n",
    "val_gen = AudioGenerator(test_df, y_te, BATCH_SIZE, shuffle=False, augment=False)\n",
    "\n",
    "# =====================\n",
    "# 4. ÊûÑÂª∫ÂèØÂæÆË∞ÉÊ®°Âûã\n",
    "# =====================\n",
    "def build_finetune_model(num_classes):\n",
    "    # 1. Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉ‰∏ª‰Ωì (Êú¨Âú∞)\n",
    "    try:\n",
    "        # from_pt=True Â¶ÇÊûú‰Ω†ÁöÑÊú¨Âú∞Êñá‰ª∂ÊòØ pytorch_model.bin\n",
    "        # Â¶ÇÊûúÊòØ tf_model.h5ÔºåÂàô‰∏çÈúÄË¶Å from_pt=True\n",
    "        # ËøôÈáåÂÅáËÆæÈÄöÁî®ÁöÑ PyTorch ÊùÉÈáç\n",
    "        backbone = TFWav2Vec2Model.from_pretrained(MODEL_CHECKPOINT, from_pt=True, local_files_only=True)\n",
    "        print(\"‚úì Model loaded from local path.\")\n",
    "    except OSError:\n",
    "        # Â∞ùËØï‰∏çÂä† from_pt (Â¶ÇÊûúÊòØ TF ÊùÉÈáç)\n",
    "        backbone = TFWav2Vec2Model.from_pretrained(MODEL_CHECKPOINT, local_files_only=True)\n",
    "    \n",
    "    # 2. === ÂÖ≥ÈîÆ === Ëß£ÂÜªÊ®°ÂûãËøõË°åÂæÆË∞É\n",
    "    backbone.trainable = True \n",
    "    \n",
    "    # 3. ÂÆö‰πâËæìÂÖ•\n",
    "    input_audio = layers.Input(shape=(TARGET_LEN,), dtype=tf.float32, name=\"audio_input\")\n",
    "    \n",
    "    # 4. ÂâçÂêë‰º†Êí≠\n",
    "    outputs = backbone(input_audio)\n",
    "    # last_hidden_state: (Batch, Time, 768)\n",
    "    x = outputs.last_hidden_state \n",
    "    \n",
    "    # 5. ÊîπËøõÁöÑÂàÜÁ±ªÂ§¥ÔºöAttention Pooling\n",
    "    # Áõ∏ÊØî GlobalAveragePoolingÔºåAttention ËÉΩËá™Âä®ÂÖ≥Ê≥®ÊÉÖÊÑüÂº∫ÁÉàÁöÑÁâáÊÆµ\n",
    "    # ÊäïÂΩ±Âà∞ËæÉ‰ΩéÁª¥Â∫¶ËÆ°ÁÆó score\n",
    "    att_score = layers.Dense(1, activation=\"tanh\")(x) # (B, T, 1)\n",
    "    att_score = layers.Flatten()(att_score)           # (B, T)\n",
    "    att_weight = layers.Activation(\"softmax\")(att_score) # (B, T)\n",
    "    \n",
    "    # Âä†ÊùÉÊ±ÇÂíå\n",
    "    # Expand dims for broadcasting: (B, T, 1) * (B, T, 768)\n",
    "    att_weight = layers.Reshape((-1, 1))(att_weight)\n",
    "    x = layers.Multiply()([x, att_weight])\n",
    "    x = tf.reduce_sum(x, axis=1) # (B, 768)\n",
    "    \n",
    "    # MLP\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x) # ÂæÆË∞ÉÊó∂ Dropout ‰∏çÂÆúËøáÂ§ß\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    \n",
    "    # Output\n",
    "    prediction = layers.Dense(num_classes, activation=\"softmax\", name=\"emotion\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=input_audio, outputs=prediction)\n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_finetune_model(len(le.classes_))\n",
    "model.summary()\n",
    "\n",
    "# =====================\n",
    "# 5. ÁºñËØë‰∏éËÆ≠ÁªÉ\n",
    "# =====================\n",
    "# ‚ö†Ô∏è ÂÖ≥ÈîÆËÆæÁΩÆÔºöÂ≠¶‰π†ÁéáÂøÖÈ°ªÈùûÂ∏∏Â∞è (1e-5)\n",
    "# Â¶ÇÊûúÂ§™Â§ßÔºå‰ºöÁû¨Èó¥Á†¥ÂùèÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºåÂØºËá¥ Loss ‰∏çÈôçÂèçÂçá\n",
    "optimizer = optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "ckpt = callbacks.ModelCheckpoint(\n",
    "    f\"{EXP_DIR}/models/best_wav2vec.h5\", \n",
    "    monitor=\"val_accuracy\", \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, # ÂØπ‰∫é HuggingFace Ê®°ÂûãÔºåÂè™‰øùÂ≠òÊùÉÈáçÊúÄÂÆâÂÖ®\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Â≠¶‰π†ÁéáË°∞Âáè\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "print(\"\\nStarting Fine-Tuning (This will be slower)...\")\n",
    "# ÂæÆË∞ÉÈÄöÂ∏∏Âè™ÈúÄË¶ÅÂæàÂ∞ëÁöÑ Epoch (5-10) Â∞±ËÉΩÊî∂Êïõ\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=15, \n",
    "    callbacks=[ckpt, lr_schedule, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# 6. ËØÑ‰º∞\n",
    "# =====================\n",
    "print(\"Evaluating...\")\n",
    "# ÈáçÊñ∞Âä†ËΩΩÊúÄ‰Ω≥ÊùÉÈáç\n",
    "model.load_weights(f\"{EXP_DIR}/models/best_wav2vec.h5\")\n",
    "\n",
    "y_pred_indices = []\n",
    "y_true_indices = []\n",
    "\n",
    "# ÊâπÈáèÈ¢ÑÊµã\n",
    "for i in range(len(val_gen)):\n",
    "    x_batch, y_batch = val_gen[i]\n",
    "    preds = model.predict(x_batch, verbose=0)\n",
    "    y_pred_indices.extend(np.argmax(preds, axis=1))\n",
    "    y_true_indices.extend(np.argmax(y_batch, axis=1))\n",
    "\n",
    "print(classification_report(y_true_indices, y_pred_indices, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93045db7-7a4b-4983-a67f-90abb80a5237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 10:14:28.303361: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 10:14:28.370076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-04 10:14:29.309254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-12-04 10:14:33.240851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31132 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:65:03.0, compute capability: 7.0\n",
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5890, Test: 1552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2025-12-04 10:14:34.115625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
      "2025-12-04 10:14:34.518323: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55eacbd41650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-04 10:14:34.518357: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2025-12-04 10:14:34.872472: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFWav2Vec2Model were not initialized from the PyTorch model and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 48000)]           0         \n",
      "                                                                 \n",
      " tf_wav2_vec2_model (TFWav2  TFWav2Vec2BaseModelOutp   94371712  \n",
      " Vec2Model)                  ut(last_hidden_state=(N             \n",
      "                             one, 149, 768),                     \n",
      "                              extract_features=(None             \n",
      "                             , 149, 512),                        \n",
      "                              hidden_states=None, at             \n",
      "                             tentions=None)                      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 149, 256)          196864    \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 149, 256)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 149, 256)          394240    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               164352    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 95135814 (362.91 MB)\n",
      "Trainable params: 95135814 (362.91 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting Advanced Fine-Tuning (60 Epochs)...\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.3/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/final_layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/final_layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.1/final_layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.5/final_layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/k_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/k_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/q_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/q_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/v_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/v_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/out_proj/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/attention/out_proj/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/layer_norm/beta:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/feed_forward/intermediate_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/feed_forward/intermediate_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/feed_forward/output_dense/kernel:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/feed_forward/output_dense/bias:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/final_layer_norm/gamma:0', 'tf_wav2_vec2_model/wav2vec2/encoder/layers.11/final_layer_norm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 10:15:24.764779: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - ETA: 0s - loss: 1.5605 - accuracy: 0.3659\n",
      "Epoch 1: val_accuracy improved from -inf to 0.29832, saving model to Output/wav2vec2_lstm_20251204_101433/models/best.h5\n",
      "93/93 [==============================] - 167s 1s/step - loss: 1.5605 - accuracy: 0.3659 - val_loss: 1.6984 - val_accuracy: 0.2983 - lr: 5.0000e-05\n",
      "Epoch 2/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 1.0449 - accuracy: 0.6311\n",
      "Epoch 3: val_accuracy improved from 0.35760 to 0.39111, saving model to Output/wav2vec2_lstm_20251204_101433/models/best.h5\n",
      "93/93 [==============================] - 101s 1s/step - loss: 1.0449 - accuracy: 0.6311 - val_loss: 1.6073 - val_accuracy: 0.3911 - lr: 5.0000e-05\n",
      "Epoch 4/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.9778 - accuracy: 0.6637\n",
      "Epoch 4: val_accuracy did not improve from 0.39111\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.9778 - accuracy: 0.6637 - val_loss: 1.5968 - val_accuracy: 0.3731 - lr: 5.0000e-05\n",
      "Epoch 5/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.8793 - accuracy: 0.6963\n",
      "Epoch 5: val_accuracy improved from 0.39111 to 0.41302, saving model to Output/wav2vec2_lstm_20251204_101433/models/best.h5\n",
      "93/93 [==============================] - 101s 1s/step - loss: 0.8793 - accuracy: 0.6963 - val_loss: 1.5789 - val_accuracy: 0.4130 - lr: 5.0000e-05\n",
      "Epoch 6/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7719 - accuracy: 0.7382\n",
      "Epoch 6: val_accuracy improved from 0.41302 to 0.52771, saving model to Output/wav2vec2_lstm_20251204_101433/models/best.h5\n",
      "93/93 [==============================] - 102s 1s/step - loss: 0.7719 - accuracy: 0.7382 - val_loss: 1.4602 - val_accuracy: 0.5277 - lr: 5.0000e-05\n",
      "Epoch 7/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6779 - accuracy: 0.7725\n",
      "Epoch 7: val_accuracy improved from 0.52771 to 0.53028, saving model to Output/wav2vec2_lstm_20251204_101433/models/best.h5\n",
      "93/93 [==============================] - 101s 1s/step - loss: 0.6779 - accuracy: 0.7725 - val_loss: 1.4360 - val_accuracy: 0.5303 - lr: 5.0000e-05\n",
      "Epoch 8/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6003 - accuracy: 0.7985\n",
      "Epoch 8: val_accuracy did not improve from 0.53028\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.6003 - accuracy: 0.7985 - val_loss: 1.4484 - val_accuracy: 0.5019 - lr: 5.0000e-05\n",
      "Epoch 9/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.8256\n",
      "Epoch 9: val_accuracy did not improve from 0.53028\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.5313 - accuracy: 0.8256 - val_loss: 1.4302 - val_accuracy: 0.5110 - lr: 5.0000e-05\n",
      "Epoch 10/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.8365\n",
      "Epoch 10: val_accuracy did not improve from 0.53028\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.5009 - accuracy: 0.8365 - val_loss: 1.4146 - val_accuracy: 0.5116 - lr: 5.0000e-05\n",
      "Epoch 11/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.8295\n",
      "Epoch 11: val_accuracy improved from 0.53028 to 0.56959, saving model to Output/wav2vec2_lstm_20251204_101433/models/best.h5\n",
      "93/93 [==============================] - 101s 1s/step - loss: 0.5211 - accuracy: 0.8295 - val_loss: 1.3701 - val_accuracy: 0.5696 - lr: 5.0000e-05\n",
      "Epoch 12/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4082 - accuracy: 0.8688\n",
      "Epoch 12: val_accuracy did not improve from 0.56959\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.4082 - accuracy: 0.8688 - val_loss: 1.3504 - val_accuracy: 0.5670 - lr: 5.0000e-05\n",
      "Epoch 13/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8689\n",
      "Epoch 13: val_accuracy improved from 0.56959 to 0.61211, saving model to Output/wav2vec2_lstm_20251204_101433/models/best.h5\n",
      "93/93 [==============================] - 101s 1s/step - loss: 0.4028 - accuracy: 0.8689 - val_loss: 1.2907 - val_accuracy: 0.6121 - lr: 5.0000e-05\n",
      "Epoch 14/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.9019\n",
      "Epoch 14: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.3104 - accuracy: 0.9019 - val_loss: 1.3225 - val_accuracy: 0.5586 - lr: 5.0000e-05\n",
      "Epoch 15/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9024\n",
      "Epoch 15: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.3068 - accuracy: 0.9024 - val_loss: 1.3007 - val_accuracy: 0.5677 - lr: 5.0000e-05\n",
      "Epoch 16/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9139\n",
      "Epoch 16: val_accuracy did not improve from 0.61211\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.2862 - accuracy: 0.9139 - val_loss: 1.3190 - val_accuracy: 0.5619 - lr: 5.0000e-05\n",
      "Epoch 17/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9390\n",
      "Epoch 17: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.1993 - accuracy: 0.9390 - val_loss: 1.2737 - val_accuracy: 0.5793 - lr: 2.5000e-05\n",
      "Epoch 18/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9486\n",
      "Epoch 18: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.1749 - accuracy: 0.9486 - val_loss: 1.2694 - val_accuracy: 0.5747 - lr: 2.5000e-05\n",
      "Epoch 19/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9526\n",
      "Epoch 19: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 100s 1s/step - loss: 0.1678 - accuracy: 0.9526 - val_loss: 1.2721 - val_accuracy: 0.5805 - lr: 2.5000e-05\n",
      "Epoch 20/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9621\n",
      "Epoch 20: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.1410 - accuracy: 0.9621 - val_loss: 1.2666 - val_accuracy: 0.5838 - lr: 2.5000e-05\n",
      "Epoch 21/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9565\n",
      "Epoch 21: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 99s 1s/step - loss: 0.1517 - accuracy: 0.9565 - val_loss: 1.2619 - val_accuracy: 0.5818 - lr: 2.5000e-05\n",
      "Epoch 22/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9572\n",
      "Epoch 22: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 100s 1s/step - loss: 0.1458 - accuracy: 0.9572 - val_loss: 1.2775 - val_accuracy: 0.5638 - lr: 2.5000e-05\n",
      "Epoch 23/60\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9638\n",
      "Epoch 23: val_accuracy did not improve from 0.61211\n",
      "93/93 [==============================] - 100s 1s/step - loss: 0.1292 - accuracy: 0.9638 - val_loss: 1.2209 - val_accuracy: 0.6057 - lr: 2.5000e-05\n",
      "Evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG     0.6988    0.8491    0.7666       265\n",
      "         DIS     0.7982    0.3434    0.4802       265\n",
      "         FEA     0.5676    0.5547    0.5611       265\n",
      "         HAP     0.7500    0.4528    0.5647       265\n",
      "         NEU     0.4964    0.9163    0.6440       227\n",
      "         SAD     0.5719    0.6000    0.5856       265\n",
      "\n",
      "    accuracy                         0.6121      1552\n",
      "   macro avg     0.6472    0.6194    0.6004      1552\n",
      "weighted avg     0.6508    0.6121    0.5993      1552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Wav2Vec 2.0 Fine-Tuning (Advanced)\n",
    "# Êû∂ÊûÑ: Raw Audio -> Wav2Vec2 (Fine-tune) -> BiLSTM -> Attention -> Emotion\n",
    "# ÊîπËøõ: Êõ¥Âº∫ÁöÑÂàÜÁ±ªÂ§¥ + Êõ¥ÈïøÁöÑËÆ≠ÁªÉ + ÂàÜÂ±ÇÂ≠¶‰π†Áéá\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "from transformers import TFWav2Vec2Model, Wav2Vec2Processor\n",
    "\n",
    "# ÊòæÂ≠òÈÖçÁΩÆ\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e: print(e)\n",
    "\n",
    "# 1. ÈÖçÁΩÆ\n",
    "MODEL_CHECKPOINT = \"wav2vec2_local\" \n",
    "AUDIO_DIR = Path(\"../AudioWAV\")\n",
    "BATCH_SIZE = 64  # ‰øùÊåÅ 8 ‰ª•Èò≤ OOM\n",
    "MAX_DURATION = 3.0 \n",
    "SR = 16000\n",
    "\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = f\"Output/wav2vec2_lstm_{timestamp}\"\n",
    "os.makedirs(f\"{EXP_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# 2. Êï∞ÊçÆÂáÜÂ§á\n",
    "def build_metadata(audio_dir):\n",
    "    records = []\n",
    "    for wav in audio_dir.glob(\"*.wav\"):\n",
    "        p = wav.stem.split(\"_\")\n",
    "        if len(p) == 4: records.append({\"path\": str(wav), \"spk\": p[0], \"emo\": p[2]})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "meta = build_metadata(AUDIO_DIR)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(meta['emo'])\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, te_idx = next(gss.split(meta, y, groups=meta['spk']))\n",
    "train_df, test_df = meta.iloc[tr_idx], meta.iloc[te_idx]\n",
    "y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# 3. Generator\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_CHECKPOINT, local_files_only=True)\n",
    "TARGET_LEN = int(SR * MAX_DURATION)\n",
    "\n",
    "class AudioGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, y, batch_size=8, shuffle=True, augment=False):\n",
    "        self.df, self.y = df, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "        self.augment = augment\n",
    "        self.indexes = np.arange(len(df))\n",
    "        if shuffle: np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self): return int(np.ceil(len(self.df)/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inds = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_x, batch_y = [], self.y[inds]\n",
    "        \n",
    "        for p in self.df.iloc[inds]['path'].values:\n",
    "            speech, _ = librosa.load(p, sr=SR)\n",
    "            # Augment\n",
    "            if self.augment and len(speech) > TARGET_LEN:\n",
    "                off = np.random.randint(0, len(speech)-TARGET_LEN)\n",
    "                speech = speech[off:off+TARGET_LEN]\n",
    "            else:\n",
    "                if len(speech) > TARGET_LEN: speech = speech[:TARGET_LEN]\n",
    "                else: speech = np.pad(speech, (0, TARGET_LEN-len(speech)))\n",
    "            batch_x.append(speech)\n",
    "            \n",
    "        inputs = processor(batch_x, sampling_rate=SR, return_tensors=\"tf\", padding=True)\n",
    "        return inputs.input_values, tf.keras.utils.to_categorical(batch_y, num_classes=len(le.classes_))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: np.random.shuffle(self.indexes)\n",
    "\n",
    "train_gen = AudioGenerator(train_df, y_tr, BATCH_SIZE, augment=True)\n",
    "val_gen = AudioGenerator(test_df, y_te, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 4. Ê®°ÂûãÊûÑÂª∫ (Wav2Vec + BiLSTM)\n",
    "def build_model(num_classes):\n",
    "    # Backbone\n",
    "    backbone = TFWav2Vec2Model.from_pretrained(MODEL_CHECKPOINT, from_pt=True, local_files_only=True)\n",
    "    backbone.trainable = True # ÂæÆË∞É\n",
    "    \n",
    "    inp = layers.Input(shape=(TARGET_LEN,), dtype=tf.float32)\n",
    "    x = backbone(inp).last_hidden_state # (B, T, 768)\n",
    "    \n",
    "    # --- Â¢ûÂº∫ÁöÑÂàÜÁ±ªÂ§¥ ---\n",
    "    # 1. ÈôçÁª¥ (ËäÇÁúÅÊòæÂ≠ò)\n",
    "    x = layers.Dense(256)(x) \n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    # 2. Bi-LSTM (ÊçïÊçâÊó∂Â∫è‰æùËµñÔºåËß£ÂÜ≥ SAD/DIS)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "    \n",
    "    # 3. Classifier\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    return models.Model(inputs=inp, outputs=out)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model(len(le.classes_))\n",
    "model.summary()\n",
    "\n",
    "# 5. ËÆ≠ÁªÉ\n",
    "# Â≠¶‰π†ÁéáËÆæÁΩÆÔºöËøôÈáåÊàë‰ª¨Áî®‰∏Ä‰∏™Á®çÂ§ßÁöÑ LR (5e-5)ÔºåÈÖçÂêà Warmup\n",
    "optimizer = optimizers.AdamW(learning_rate=5e-5, weight_decay=1e-4)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Callbacks\n",
    "ckpt = callbacks.ModelCheckpoint(f\"{EXP_DIR}/models/best.h5\", monitor=\"val_accuracy\", save_best_only=True, save_weights_only=True, verbose=1)\n",
    "# Â¢ûÂä†ËÄêÂøÉÂÄº\n",
    "early = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True)\n",
    "# LR Ë°∞Âáè\n",
    "reduce = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-7)\n",
    "\n",
    "print(\"\\nStarting Advanced Fine-Tuning (60 Epochs)...\")\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=60, # Â¢ûÂä†ËÆ≠ÁªÉËΩÆÊï∞\n",
    "    callbacks=[ckpt, early, reduce],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 6. ËØÑ‰º∞\n",
    "print(\"Evaluating...\")\n",
    "model.load_weights(f\"{EXP_DIR}/models/best.h5\")\n",
    "yp, yt = [], []\n",
    "for i in range(len(val_gen)):\n",
    "    x, y = val_gen[i]\n",
    "    p = model.predict(x, verbose=0)\n",
    "    yp.extend(np.argmax(p, axis=1))\n",
    "    yt.extend(np.argmax(y, axis=1))\n",
    "\n",
    "print(classification_report(yt, yp, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fbe05-ee9d-4b0e-9e47-3561d572e4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
