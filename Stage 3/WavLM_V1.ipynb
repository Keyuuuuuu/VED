{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1868dd-007c-4729-8fa5-37eb90189b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/commands/download.py:141: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "\u001B[33m‚ö†Ô∏è  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001B[0m\n",
      "/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 5 files:   0%|                                   | 0/5 [00:00<?, ?it/s]Downloading 'pytorch_model.bin' to 'wavlm_base_plus_local/.cache/huggingface/download/Q1p2l2BzM1m6P5jKvr8WTq1TUio=.3bb273a6ace99408b50cfc81afdbb7ef2de02da2eab0234e18db608ce692fe51.incomplete'\n",
      "Downloading 'README.md' to 'wavlm_base_plus_local/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.1192671605727eff94113d3b599161892688b9d5.incomplete'\n",
      "Downloading '.gitattributes' to 'wavlm_base_plus_local/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.6d34772f5ca361021038b404fb913ec8dc0b1a5a.incomplete'\n",
      "Downloading 'config.json' to 'wavlm_base_plus_local/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.b7b4e5f6c410f71283a59e26250e40855ca99310.incomplete'\n",
      "Downloading 'preprocessor_config.json' to 'wavlm_base_plus_local/.cache/huggingface/download/PYH5dHjks7Ei0Yd3X0Z8xIwsCNQ=.10f6def8c83d70a2b087a567dcf523b75152a80b.incomplete'\n",
      "\n",
      "config.json: 2.23kB [00:00, 4.68MB/s]\n",
      "Download complete. Moving file to wavlm_base_plus_local/config.json\n",
      "\n",
      "preprocessor_config.json: 215B [00:00, 818kB/s]A\n",
      "Download complete. Moving file to wavlm_base_plus_local/preprocessor_config.json\n",
      "\n",
      "pytorch_model.bin:   0%|                             | 0.00/378M [00:00<?, ?B/s]\u001B[A\n",
      "\n",
      "README.md: 3.90kB [00:00, 15.1MB/s]A\n",
      "Download complete. Moving file to wavlm_base_plus_local/README.md\n",
      "\n",
      "\n",
      ".gitattributes: 1.18kB [00:00, 4.13MB/s]A\n",
      "Download complete. Moving file to wavlm_base_plus_local/.gitattributes\n",
      "Fetching 5 files:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1/5 [00:01<00:05,  1.27s/it]\n",
      "pytorch_model.bin:   3%|‚ñå                   | 10.5M/378M [00:01<00:49, 7.42MB/s]\u001B[A\n",
      "pytorch_model.bin:   6%|‚ñà                   | 21.0M/378M [00:01<00:30, 11.8MB/s]\u001B[A\n",
      "pytorch_model.bin:   8%|‚ñà‚ñã                  | 31.5M/378M [00:02<00:23, 14.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  11%|‚ñà‚ñà‚ñè                 | 41.9M/378M [00:03<00:25, 13.1MB/s]\u001B[A\n",
      "pytorch_model.bin:  14%|‚ñà‚ñà‚ñä                 | 52.4M/378M [00:04<00:26, 12.3MB/s]\u001B[A\n",
      "pytorch_model.bin:  17%|‚ñà‚ñà‚ñà‚ñé                | 62.9M/378M [00:05<00:25, 12.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  19%|‚ñà‚ñà‚ñà‚ñâ                | 73.4M/378M [00:06<00:25, 12.0MB/s]\u001B[A\n",
      "pytorch_model.bin:  22%|‚ñà‚ñà‚ñà‚ñà‚ñç               | 83.9M/378M [00:06<00:24, 12.2MB/s]\u001B[A\n",
      "pytorch_model.bin:  25%|‚ñà‚ñà‚ñà‚ñà‚ñâ               | 94.4M/378M [00:07<00:23, 11.9MB/s]\u001B[A\n",
      "pytorch_model.bin:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 105M/378M [00:08<00:23, 11.8MB/s]\u001B[A\n",
      "pytorch_model.bin:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 115M/378M [00:09<00:22, 11.7MB/s]\u001B[A\n",
      "pytorch_model.bin:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 126M/378M [00:10<00:21, 11.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 136M/378M [00:11<00:20, 11.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 147M/378M [00:12<00:19, 11.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 157M/378M [00:13<00:19, 11.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 168M/378M [00:14<00:20, 10.0MB/s]\u001B[A\n",
      "pytorch_model.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 178M/378M [00:15<00:19, 10.4MB/s]\u001B[A\n",
      "pytorch_model.bin:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 189M/378M [00:16<00:16, 11.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 199M/378M [00:16<00:13, 13.3MB/s]\u001B[A\n",
      "pytorch_model.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 210M/378M [00:17<00:12, 13.5MB/s]\u001B[A\n",
      "pytorch_model.bin:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 220M/378M [00:18<00:11, 13.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 231M/378M [00:19<00:11, 12.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 241M/378M [00:20<00:10, 12.7MB/s]\u001B[A\n",
      "pytorch_model.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 252M/378M [00:21<00:10, 12.2MB/s]\u001B[A\n",
      "pytorch_model.bin:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 262M/378M [00:21<00:09, 11.7MB/s]\u001B[A\n",
      "pytorch_model.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 273M/378M [00:22<00:08, 12.0MB/s]\u001B[A\n",
      "pytorch_model.bin:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 283M/378M [00:23<00:07, 12.5MB/s]\u001B[A\n",
      "pytorch_model.bin:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 294M/378M [00:24<00:07, 11.9MB/s]\u001B[A\n",
      "pytorch_model.bin:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 304M/378M [00:25<00:06, 11.6MB/s]\u001B[A\n",
      "pytorch_model.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 315M/378M [00:26<00:05, 12.0MB/s]\u001B[A\n",
      "pytorch_model.bin:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 325M/378M [00:27<00:04, 11.8MB/s]\u001B[A\n",
      "pytorch_model.bin:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 336M/378M [00:28<00:04, 10.1MB/s]\u001B[A\n",
      "pytorch_model.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 346M/378M [00:29<00:03, 10.5MB/s]\u001B[A\n",
      "pytorch_model.bin:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 357M/378M [00:30<00:01, 10.8MB/s]\u001B[A\n",
      "pytorch_model.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 367M/378M [00:31<00:00, 11.0MB/s]\u001B[A\n",
      "pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 378M/378M [00:32<00:00, 11.7MB/s]\u001B[A\n",
      "Download complete. Moving file to wavlm_base_plus_local/pytorch_model.bin\n",
      "Fetching 5 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:33<00:00,  6.63s/it]\n",
      "/root/VED/wavlm_base_plus_local\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# üöÄ ‰∏ãËΩΩ Microsoft WavLM Base Plus\n",
    "# =======================================================\n",
    "!export HF_ENDPOINT=https://hf-mirror.com && huggingface-cli download \\\n",
    "    --resume-download microsoft/wavlm-base-plus \\\n",
    "    --local-dir ./wavlm_base_plus_local \\\n",
    "    --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b30e68-e989-40fe-9c2b-7a648f62ccb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 07:56:05.954208: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-06 07:56:06.022473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-06 07:56:07.247795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Classes: ['ANG' 'DIS' 'FEA' 'HAP' 'NEU' 'SAD']\n",
      "Train: 5890, Test: 1552\n",
      "Ê≠£Âú®Âä†ËΩΩ Feature Extractor: ./wavlm_base_plus_local\n",
      "‚úì Frozen CNN feature extractor and first 8 Transformer layers.\n",
      "\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 1.7656 | Acc: 24.87% | Val Acc: 35.57% | F1: 0.3052\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 | Train Loss: 1.6932 | Acc: 35.50% | Val Acc: 32.22% | F1: 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 | Train Loss: 1.5577 | Acc: 37.78% | Val Acc: 38.27% | F1: 0.2954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 | Train Loss: 1.4630 | Acc: 41.48% | Val Acc: 40.40% | F1: 0.3393\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 | Train Loss: 1.4020 | Acc: 44.06% | Val Acc: 44.01% | F1: 0.4000\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 | Train Loss: 1.3507 | Acc: 47.25% | Val Acc: 47.42% | F1: 0.4494\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 | Train Loss: 1.2392 | Acc: 53.02% | Val Acc: 53.29% | F1: 0.5152\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 | Train Loss: 1.1676 | Acc: 54.94% | Val Acc: 58.38% | F1: 0.5784\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 | Train Loss: 1.0892 | Acc: 58.71% | Val Acc: 56.12% | F1: 0.5550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 | Train Loss: 1.0640 | Acc: 59.56% | Val Acc: 61.60% | F1: 0.6102\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 | Train Loss: 0.9983 | Acc: 62.50% | Val Acc: 60.70% | F1: 0.5946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 | Train Loss: 0.9921 | Acc: 61.99% | Val Acc: 59.34% | F1: 0.5780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 | Train Loss: 0.9742 | Acc: 61.97% | Val Acc: 62.11% | F1: 0.6091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 | Train Loss: 0.9151 | Acc: 65.06% | Val Acc: 62.11% | F1: 0.6157\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 | Train Loss: 0.9287 | Acc: 63.62% | Val Acc: 61.53% | F1: 0.6054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 | Train Loss: 0.9512 | Acc: 63.85% | Val Acc: 63.79% | F1: 0.6305\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 | Train Loss: 0.9072 | Acc: 66.37% | Val Acc: 65.91% | F1: 0.6557\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 | Train Loss: 0.8590 | Acc: 67.45% | Val Acc: 67.33% | F1: 0.6706\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 | Train Loss: 0.8529 | Acc: 67.42% | Val Acc: 65.98% | F1: 0.6557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 | Train Loss: 0.8488 | Acc: 68.66% | Val Acc: 67.14% | F1: 0.6706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 | Train Loss: 0.8939 | Acc: 66.83% | Val Acc: 67.78% | F1: 0.6766\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 | Train Loss: 0.8514 | Acc: 68.49% | Val Acc: 64.50% | F1: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 | Train Loss: 0.8526 | Acc: 67.52% | Val Acc: 65.91% | F1: 0.6572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 | Train Loss: 0.8371 | Acc: 69.32% | Val Acc: 66.30% | F1: 0.6582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 | Train Loss: 0.7986 | Acc: 69.51% | Val Acc: 66.62% | F1: 0.6608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 | Train Loss: 0.8370 | Acc: 69.15% | Val Acc: 66.69% | F1: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 | Train Loss: 0.8000 | Acc: 69.24% | Val Acc: 68.23% | F1: 0.6780\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 | Train Loss: 0.7717 | Acc: 71.53% | Val Acc: 67.01% | F1: 0.6631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 | Train Loss: 0.7999 | Acc: 70.58% | Val Acc: 68.62% | F1: 0.6825\n",
      "  ‚òÖ New Best F1! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 | Train Loss: 0.7968 | Acc: 70.17% | Val Acc: 68.62% | F1: 0.6822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 | Train Loss: 0.7772 | Acc: 71.71% | Val Acc: 67.20% | F1: 0.6682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 | Train Loss: 0.7745 | Acc: 70.80% | Val Acc: 65.91% | F1: 0.6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 | Train Loss: 0.7586 | Acc: 71.80% | Val Acc: 66.75% | F1: 0.6580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 | Train Loss: 0.7872 | Acc: 72.70% | Val Acc: 68.49% | F1: 0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 | Train Loss: 0.8364 | Acc: 70.51% | Val Acc: 65.66% | F1: 0.6463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 | Train Loss: 0.7884 | Acc: 70.34% | Val Acc: 67.20% | F1: 0.6650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 | Train Loss: 0.7427 | Acc: 72.58% | Val Acc: 68.04% | F1: 0.6755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 | Train Loss: 0.7314 | Acc: 72.09% | Val Acc: 68.04% | F1: 0.6725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 | Train Loss: 0.7175 | Acc: 73.31% | Val Acc: 67.78% | F1: 0.6703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 | Train Loss: 0.7176 | Acc: 73.53% | Val Acc: 68.11% | F1: 0.6727\n",
      "\n",
      "Loading best model for final evaluation...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG     0.6829    0.9019    0.7772       265\n",
      "         DIS     0.6456    0.5774    0.6096       265\n",
      "         FEA     0.6958    0.6302    0.6614       265\n",
      "         HAP     0.6162    0.6906    0.6512       265\n",
      "         NEU     0.7760    0.8546    0.8134       227\n",
      "         SAD     0.7247    0.4868    0.5824       265\n",
      "\n",
      "    accuracy                         0.6862      1552\n",
      "   macro avg     0.6902    0.6902    0.6825      1552\n",
      "weighted avg     0.6881    0.6862    0.6793      1552\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5172/1126958488.py:324: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(range(13)), y=weights, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer weights plot saved.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Speech Emotion Recognition - WavLM Base Plus (Layer-wise Weighted)\n",
    "# Á≠ñÁï•: \n",
    "# 1. Ê®°Âûã: microsoft/wavlm-base-plus (È¢ÑËÆ≠ÁªÉÂº∫ÔºåÂèÇÊï∞ÈÄÇ‰∏≠)\n",
    "# 2. ÂÜªÁªì: CNN + Transformer Ââç8Â±Ç (Âè™ËÆ≠ÁªÉ Top 4 Layers)\n",
    "# 3. ËûçÂêà: ÊèêÂèñÊâÄÊúâ Hidden StatesÔºåÂ≠¶‰π†Â±ÇÁ∫ßÊùÉÈáç (Weighted Sum)\n",
    "# 4. Êï∞ÊçÆ: Âä®ÊÄÅ Padding/Cropping + On-the-fly Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "# Á¶ªÁ∫øÊ®°ÂºèÈÖçÁΩÆ (ÈÄÇÈÖç‰Ω†ÁöÑÊúçÂä°Âô®ÁéØÂ¢É)\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\" \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from transformers import WavLMModel\n",
    "\n",
    "# Ê£ÄÊü• GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =====================\n",
    "# 1. ÈÖçÁΩÆÂèÇÊï∞\n",
    "# =====================\n",
    "# ËØ∑Á°Æ‰øù‰∏ãËΩΩ‰∫Ü 'microsoft/wavlm-base-plus' Âà∞Êú¨Âú∞Êñá‰ª∂Â§π\n",
    "MODEL_NAME_OR_PATH = \"../wavlm_base_plus_local\"\n",
    "# Â¶ÇÊûú‰Ω†Âè™Êúâ wav2vec2-baseÔºå‰πüÂèØ‰ª•Áî®Ôºå‰ª£Á†ÅÈÄöÁî®ÔºåÂè™ÈúÄÊîπË∑ØÂæÑ\n",
    "# MODEL_NAME_OR_PATH = \"facebook/wav2vec2-base-960h\" \n",
    "\n",
    "AUDIO_DIR = Path(\"../AudioWAV\")\n",
    "OUTPUT_DIR = \"../Output/WavLM_LayerWise\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"sr\": 16000,\n",
    "    \"max_duration\": 3.0,\n",
    "    \"batch_size\": 128,     # V100 32GB ÁîöËá≥ÂèØ‰ª•ÂºÄÂà∞ 32\n",
    "    \"num_epochs\": 40,     # Â∞èÊï∞ÊçÆÁ≤æË∞ÉÔºåËΩÆÊï∞‰∏çÁî®Â§™Â§ö\n",
    "    \"lr\": 1e-4,           # ÂàÜÁ±ªÂ§¥Â≠¶‰π†Áéá\n",
    "    \"backbone_lr\": 1e-5,  #Áî±‰∫éËß£ÂÜª‰∫ÜÈ´òÂ±ÇÔºåÁªôÊûÅÂ∞èÂ≠¶‰π†Áéá\n",
    "    \"freeze_layers\": 8,   # ÂÜªÁªìÂâç 8 Â±Ç Transformer\n",
    "    \"num_classes\": 6\n",
    "}\n",
    "\n",
    "# =====================\n",
    "# 2. Êï∞ÊçÆÂáÜÂ§á\n",
    "# =====================\n",
    "def build_metadata(audio_dir):\n",
    "    records = []\n",
    "    for wav in audio_dir.glob(\"*.wav\"):\n",
    "        parts = wav.stem.split(\"_\")\n",
    "        if len(parts) == 4:\n",
    "            records.append({\"path\": str(wav), \"spk\": parts[0], \"emo\": parts[2]})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "meta = build_metadata(AUDIO_DIR)\n",
    "le = LabelEncoder()\n",
    "meta['label'] = le.fit_transform(meta['emo'])\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "\n",
    "# ÂàíÂàÜ (Speaker Independent)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, te_idx = next(gss.split(meta, meta['label'], groups=meta['spk']))\n",
    "train_df = meta.iloc[tr_idx].reset_index(drop=True)\n",
    "test_df = meta.iloc[te_idx].reset_index(drop=True)\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# =====================\n",
    "# 3. Dataset & Collator (Âê´Êï∞ÊçÆÂ¢ûÂº∫)\n",
    "# =====================\n",
    "\n",
    "\n",
    "class CREMADataset(Dataset):\n",
    "    def __init__(self, df, processor, sr=16000, duration=3.0, augment=False):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.sr = sr\n",
    "        self.target_len = int(sr * duration)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        # Âä†ËΩΩÈü≥È¢ë\n",
    "        wav, _ = librosa.load(path, sr=self.sr)\n",
    "        \n",
    "        # 1. ÈïøÂ∫¶Ê†áÂáÜÂåñ (Center Crop / Pad)\n",
    "        if len(wav) > self.target_len:\n",
    "            # ËÆ≠ÁªÉÊó∂ÈöèÊú∫Ë£ÅÂâ™ÔºåÊµãËØïÊó∂‰∏≠ÂøÉË£ÅÂâ™\n",
    "            if self.augment:\n",
    "                start = np.random.randint(0, len(wav) - self.target_len)\n",
    "            else:\n",
    "                start = (len(wav) - self.target_len) // 2\n",
    "            wav = wav[start : start + self.target_len]\n",
    "        else:\n",
    "            pad_len = self.target_len - len(wav)\n",
    "            wav = np.pad(wav, (0, pad_len), mode='constant')\n",
    "\n",
    "        # 2. Êï∞ÊçÆÂ¢ûÂº∫ (‰ªÖËÆ≠ÁªÉ)\n",
    "        if self.augment:\n",
    "            # ÊûÅ‰ΩéÊ¶ÇÁéáÂä†È´òÊñØÂô™\n",
    "            if np.random.random() < 0.3:\n",
    "                noise = np.random.randn(len(wav))\n",
    "                wav += 0.005 * noise\n",
    "\n",
    "        # Processor Â§ÑÁêÜ (ËøîÂõû tensor)\n",
    "        # Ê≥®ÊÑèÔºöËøôÈáåÊàë‰ª¨‰∏çËÆ© processor ÂÅö paddingÔºåËÄåÂú® collate_fn ÈáåÂÅö batch padding\n",
    "        inputs = self.processor(wav, sampling_rate=self.sr, return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": inputs.input_values.squeeze(0), # (T,)\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # ÊâãÂä®Â†ÜÂè†ÔºåÂõ†‰∏∫Â∑≤ÁªèÁªü‰∏ÄÈïøÂ∫¶‰∫Ü\n",
    "    input_values = torch.stack([item['input_values'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# Âä†ËΩΩ Processor (Â∞ùËØïÊú¨Âú∞Âä†ËΩΩ)\n",
    "print(f\"Ê≠£Âú®Âä†ËΩΩ Feature Extractor: {MODEL_NAME_OR_PATH}\")\n",
    "\n",
    "# ‚úÖ Ê†∏ÂøÉ‰øÆÊîπÔºö‰ΩøÁî® Wav2Vec2FeatureExtractor ËÄå‰∏çÊòØ Processor\n",
    "# ËøôÊ†∑ÂÆÉÂ∞±‰∏ç‰ºöÂéªÊ£ÄÊü• tokenizer ‰∫ÜÔºåÂè™‰ºöËØªÂèñ config.json Âíå preprocessor_config.json\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME_OR_PATH, local_files_only=True)\n",
    "\n",
    "# ‰Ω†ÁöÑ Dataset Á±ª‰∏çÈúÄË¶ÅÊîπÂä®ÔºåÂõ†‰∏∫ FeatureExtractor ÁöÑÁî®Ê≥ïÂíå Processor ÊòØ‰∏ÄÊ®°‰∏ÄÊ†∑ÁöÑ\n",
    "train_ds = CREMADataset(train_df, processor, augment=True)\n",
    "test_ds = CREMADataset(test_df, processor, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "# =====================\n",
    "# 4. Ê®°ÂûãÊû∂ÊûÑ: WavLM + Weighted Layer Sum\n",
    "# =====================\n",
    "class WavLMEmoModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, freeze_layers=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Âä†ËΩΩ Backbone\n",
    "        # output_hidden_states=True ÊòØÂøÖÈ°ªÁöÑÔºå‰∏∫‰∫ÜÊãøÂà∞ÊâÄÊúâÂ±Ç\n",
    "        try:\n",
    "            self.wavlm = WavLMModel.from_pretrained(model_name, local_files_only=True, output_hidden_states=True)\n",
    "        except:\n",
    "            self.wavlm = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\", output_hidden_states=True)\n",
    "            \n",
    "        # 2. ÂÜªÁªìÁ≠ñÁï•\n",
    "        self.wavlm.feature_extractor._freeze_parameters() # ÂÜªÁªì CNN\n",
    "        \n",
    "        # ÂÜªÁªìÂâç N Â±Ç Transformer\n",
    "        for layer in self.wavlm.encoder.layers[:freeze_layers]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        print(f\"‚úì Frozen CNN feature extractor and first {freeze_layers} Transformer layers.\")\n",
    "        \n",
    "        # 3. Â±ÇÁ∫ßÂä†ÊùÉÂèÇÊï∞ (Learnable Weights)\n",
    "        # WavLM Base Êúâ 13 Â±ÇËæìÂá∫ (Embedding + 12 Layers)\n",
    "        self.layer_weights = nn.Parameter(torch.ones(13) / 13) \n",
    "        \n",
    "        # 4. ÂàÜÁ±ªÂ§¥\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        # outputs.hidden_states ÊòØ‰∏Ä‰∏™ tupleÔºåÂåÖÂê´ 13 ‰∏™ tensor\n",
    "        # ÊØè‰∏™ tensor shape: (Batch, Time, 768)\n",
    "        outputs = self.wavlm(input_values)\n",
    "        \n",
    "        # Stack hidden states -> (Batch, Time, 768, 13)\n",
    "        # ÊîæÂà∞ GPU ‰∏äÊìç‰Ωú\n",
    "        all_layers = torch.stack(outputs.hidden_states, dim=-1)\n",
    "        \n",
    "        # ËÆ°ÁÆóÂä†ÊùÉÊùÉÈáç (Softmax ÂΩí‰∏ÄÂåñÔºå‰øùËØÅÂíå‰∏∫1)\n",
    "        norm_weights = F.softmax(self.layer_weights, dim=0)\n",
    "        \n",
    "        # Âä†ÊùÉÊ±ÇÂíå -> (Batch, Time, 768)\n",
    "        # sum( H_i * w_i )\n",
    "        weighted_feature = (all_layers * norm_weights).sum(dim=-1)\n",
    "        \n",
    "        # Mean Pooling over Time -> (Batch, 768)\n",
    "        # ËøôÈáåÁî® Mean Pooling ÈÖçÂêàÂ±ÇÁ∫ßÂä†ÊùÉÊïàÊûúÈÄöÂ∏∏ÊúÄÂ•ΩÔºå\n",
    "        # Âõ†‰∏∫Â±ÇÁ∫ßÂä†ÊùÉÂ∑≤ÁªèÊèêÂèñ‰∫ÜÈüµÂæã‰ø°ÊÅØÔºå‰∏çÈúÄË¶ÅÂÜçÂÅö Attention Pooling ‰∫Ü\n",
    "        pooled_feature = weighted_feature.mean(dim=1)\n",
    "        \n",
    "        # Classifier\n",
    "        logits = self.projector(pooled_feature)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# =====================\n",
    "# 5. ËÆ≠ÁªÉÂæ™ÁéØ\n",
    "# =====================\n",
    "from tqdm import tqdm  \n",
    "\n",
    "model = WavLMEmoModel(MODEL_NAME_OR_PATH, CONFIG['num_classes'], freeze_layers=CONFIG['freeze_layers'])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.wavlm.parameters(), 'lr': CONFIG['backbone_lr']},\n",
    "    {'params': model.layer_weights, 'lr': 1e-3},\n",
    "    {'params': model.projector.parameters(), 'lr': CONFIG['lr']}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_f1 = 0.0\n",
    "save_path = os.path.join(OUTPUT_DIR, \"best_wavlm.pth\")\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # üî• ‰øÆÊîπÁÇπ 1: Áî® tqdm ÂåÖË£π train_loader\n",
    "    # descÊòæÁ§∫ÂΩìÂâçEpochÔºåleave=FalseË°®Á§∫Ë∑ëÂÆåÂêéÊ∏ÖÈô§ËøõÂ∫¶Êù°ÔºàÈÅøÂÖçÂà∑Â±èÔºâ\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\", leave=False)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        inputs = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ËÆ∞ÂΩïÁªüËÆ°\n",
    "        current_loss = loss.item()\n",
    "        train_loss += current_loss\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # üî• ‰øÆÊîπÁÇπ 2: ÂÆûÊó∂Êõ¥Êñ∞ËøõÂ∫¶Êù°Âè≥‰æßÁöÑÂ∞èÂ∞æÂ∑¥ (ÊòæÁ§∫ÂΩìÂâç Batch ÁöÑ Loss)\n",
    "        pbar.set_postfix({\"loss\": f\"{current_loss:.4f}\"})\n",
    "    \n",
    "    # ËÆ°ÁÆó Epoch Á∫ßÂà´ÁöÑÂπ≥ÂùáÊåáÊ†á\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    # È™åËØÅÈõÜÈÄöÂ∏∏ÂæàÂø´Ôºå‰∏çÈúÄË¶ÅËøõÂ∫¶Êù°ÔºåÊàñËÄÖÁÆÄÂçïÂä†‰∏Ä‰∏™‰πüË°å\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_acc = np.mean(np.array(val_preds) == np.array(val_labels))\n",
    "    \n",
    "    # ÊâìÂç∞ÊúÄÁªàÁªìÊûú\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['num_epochs']} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}% | F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save Best\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  ‚òÖ New Best F1! Model saved.\")\n",
    "\n",
    "# =====================\n",
    "# 6. ÊúÄÁªàËØÑ‰º∞\n",
    "# =====================\n",
    "print(\"\\nLoading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_, digits=4))\n",
    "\n",
    "# ÁªòÂà∂Â±ÇÁ∫ßÊùÉÈáçÁÉ≠ÂäõÂõæ\n",
    "weights = F.softmax(model.layer_weights, dim=0).detach().cpu().numpy()\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x=list(range(13)), y=weights, palette=\"viridis\")\n",
    "plt.title(\"Learned Layer Weights (Importance)\")\n",
    "plt.xlabel(\"Layer Index (0=Embed, 12=Top)\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"layer_weights.png\"))\n",
    "print(\"Layer weights plot saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5777d-6565-4b9d-88a6-20e51597ecc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
