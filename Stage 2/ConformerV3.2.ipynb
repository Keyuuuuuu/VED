{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a80cd8-9042-41da-9c45-23d9aa770ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Experiment directory: Output/conformer_v3_2_notebook_20251204_064339\n",
      "Samples: 7442\n",
      "Feature shape: (300, 80, 1)\n",
      "Extracting features (80 Mels)...\n",
      "X shape: (7442, 300, 80, 1)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 300, 80, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " gaussian_noise_1 (Gaussian  (None, 300, 80, 1)           0         ['input_2[0][0]']             \n",
      " Noise)                                                                                           \n",
      "                                                                                                  \n",
      " spec_augment_1 (SpecAugmen  (None, 300, 80, 1)           0         ['gaussian_noise_1[0][0]']    \n",
      " t)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 300, 80, 32)          320       ['spec_augment_1[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 300, 80, 32)          832       ['spec_augment_1[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 300, 80, 32)          1600      ['spec_augment_1[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 300, 80, 96)          0         ['conv2d_4[0][0]',            \n",
      " )                                                                   'conv2d_5[0][0]',            \n",
      "                                                                     'conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 300, 80, 96)          384       ['concatenate_1[0][0]']       \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1  (None, 96)                   0         ['batch_normalization_2[0][0]'\n",
      "  (GlobalAveragePooling2D)                                          ]                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 12)                   1164      ['global_average_pooling2d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 96)                   1248      ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 1, 1, 96)             0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)       (None, 300, 80, 96)          0         ['batch_normalization_2[0][0]'\n",
      "                                                                    , 'reshape_1[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 150, 40, 96)          0         ['multiply_1[0][0]']          \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 150, 40, 128)         110720    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 150, 40, 128)         512       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2  (None, 128)                  0         ['batch_normalization_3[0][0]'\n",
      "  (GlobalAveragePooling2D)                                          ]                             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 16)                   2064      ['global_average_pooling2d_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 128)                  2176      ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 1, 1, 128)            0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)       (None, 150, 40, 128)         0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'reshape_2[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 75, 20, 128)          0         ['multiply_2[0][0]']          \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " spatial_dropout2d (Spatial  (None, 75, 20, 128)          0         ['max_pooling2d_2[0][0]']     \n",
      " Dropout2D)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 75, 20, 256)          295168    ['spatial_dropout2d[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 75, 20, 256)          1024      ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3  (None, 256)                  0         ['batch_normalization_4[0][0]'\n",
      "  (GlobalAveragePooling2D)                                          ]                             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 32)                   8224      ['global_average_pooling2d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 256)                  8448      ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)         (None, 1, 1, 256)            0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)       (None, 75, 20, 256)          0         ['batch_normalization_4[0][0]'\n",
      "                                                                    , 'reshape_3[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 37, 10, 256)          0         ['multiply_3[0][0]']          \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " spatial_dropout2d_1 (Spati  (None, 37, 10, 256)          0         ['max_pooling2d_3[0][0]']     \n",
      " alDropout2D)                                                                                     \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)         (None, 37, 2560)             0         ['spatial_dropout2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 37, 144)              368784    ['reshape_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 37, 144)              0         ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 37, 576)              83520     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 37, 576)              0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 37, 144)              83088     ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 37, 144)              0         ['dense_10[0][0]']            \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 37, 144)              0         ['dropout[0][0]',             \n",
      " Lambda)                                                             'tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 37, 144)              288       ['tf.__operators__.add[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 37, 144)              83520     ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 37, 144)              0         ['layer_normalization[0][0]', \n",
      " OpLambda)                                                           'multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 37, 144)              288       ['tf.__operators__.add_1[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 37, 288)              41760     ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 37, 144)              0         ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " depthwise_conv1d (Depthwis  (None, 37, 144)              2304      ['lambda[0][0]']              \n",
      " eConv1D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 37, 144)              576       ['depthwise_conv1d[0][0]']    \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 37, 144)              0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 37, 144)              20880     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 37, 144)              0         ['conv1d_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 37, 144)              0         ['layer_normalization_1[0][0]'\n",
      " OpLambda)                                                          , 'dropout_2[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 37, 144)              288       ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 37, 576)              83520     ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 37, 576)              0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 37, 144)              83088     ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLa  (None, 37, 144)              0         ['dense_12[0][0]']            \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 37, 144)              0         ['layer_normalization_2[0][0]'\n",
      " OpLambda)                                                          , 'tf.math.multiply_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 37, 144)              288       ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 37, 576)              83520     ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 37, 576)              0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 37, 144)              83088     ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLa  (None, 37, 144)              0         ['dense_14[0][0]']            \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 37, 144)              0         ['layer_normalization_3[0][0]'\n",
      " OpLambda)                                                          , 'tf.math.multiply_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 37, 144)              288       ['tf.__operators__.add_4[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 37, 144)              83520     ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 37, 144)              0         ['layer_normalization_4[0][0]'\n",
      " OpLambda)                                                          , 'multi_head_attention_1[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 37, 144)              288       ['tf.__operators__.add_5[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 37, 288)              41760     ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 37, 144)              0         ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " depthwise_conv1d_1 (Depthw  (None, 37, 144)              4608      ['lambda_1[0][0]']            \n",
      " iseConv1D)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 37, 144)              576       ['depthwise_conv1d_1[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 37, 144)              0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 37, 144)              20880     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 37, 144)              0         ['conv1d_3[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 37, 144)              0         ['layer_normalization_5[0][0]'\n",
      " OpLambda)                                                          , 'dropout_5[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 37, 144)              288       ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 37, 576)              83520     ['layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 37, 576)              0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 37, 144)              83088     ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLa  (None, 37, 144)              0         ['dense_16[0][0]']            \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 37, 144)              0         ['layer_normalization_6[0][0]'\n",
      " OpLambda)                                                          , 'tf.math.multiply_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 37, 144)              288       ['tf.__operators__.add_7[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 37, 1)                145       ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " softmax (Softmax)           (None, 37, 1)                0         ['dense_17[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLa  (None, 37, 144)              0         ['layer_normalization_7[0][0]'\n",
      " mbda)                                                              , 'softmax[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLa  (None, 144)                  0         ['tf.math.multiply_4[0][0]']  \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 144)                  0         ['tf.math.reduce_sum[0][0]']  \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 64)                   9280      ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 64)                   0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " emotion (Dense)             (None, 6)                    390       ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1781603 (6.80 MB)\n",
      "Trainable params: 1780067 (6.79 MB)\n",
      "Non-trainable params: 1536 (6.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Starting V3.2 Training (SGDR + Large Kernel)...\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 06:44:48.857648: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/spatial_dropout2d/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2025-12-04 06:44:50.422951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2025-12-04 06:44:51.072396: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6c0090d4e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-04 06:44:51.072432: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2025-12-04 06:44:51.082628: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-04 06:44:51.260857: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/93 [>.............................] - ETA: 11s - loss: 1.5343 - accuracy: 0.1641WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0546s vs `on_train_batch_end` time: 0.0643s). Check your callbacks.\n",
      "93/93 [==============================] - ETA: 0s - loss: 1.1606 - accuracy: 0.2900 — val_f1: 0.1109\n",
      "\n",
      "Epoch 1: val_macro_f1 improved from -inf to 0.11089, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 42s 190ms/step - loss: 1.1606 - accuracy: 0.2900 - val_loss: 1.8665 - val_accuracy: 0.2055 - val_macro_f1: 0.1109 - lr: 1.6000e-04\n",
      "Epoch 2/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 1.0569 - accuracy: 0.3499 — val_f1: 0.0834\n",
      "\n",
      "Epoch 2: val_macro_f1 did not improve from 0.11089\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 1.0569 - accuracy: 0.3499 - val_loss: 2.0202 - val_accuracy: 0.1920 - val_macro_f1: 0.0834 - lr: 3.2000e-04\n",
      "Epoch 3/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 1.0390 - accuracy: 0.3745 — val_f1: 0.1235\n",
      "\n",
      "Epoch 3: val_macro_f1 improved from 0.11089 to 0.12347, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 153ms/step - loss: 1.0390 - accuracy: 0.3745 - val_loss: 1.4981 - val_accuracy: 0.2229 - val_macro_f1: 0.1235 - lr: 4.8000e-04\n",
      "Epoch 4/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 1.0140 - accuracy: 0.3900 — val_f1: 0.3013\n",
      "\n",
      "Epoch 4: val_macro_f1 improved from 0.12347 to 0.30135, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 153ms/step - loss: 1.0140 - accuracy: 0.3900 - val_loss: 1.0775 - val_accuracy: 0.3428 - val_macro_f1: 0.3013 - lr: 6.4000e-04\n",
      "Epoch 5/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.9805 - accuracy: 0.4098 — val_f1: 0.4223\n",
      "\n",
      "Epoch 5: val_macro_f1 improved from 0.30135 to 0.42231, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 153ms/step - loss: 0.9805 - accuracy: 0.4098 - val_loss: 0.9468 - val_accuracy: 0.4330 - val_macro_f1: 0.4223 - lr: 8.0000e-04\n",
      "Epoch 6/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.9369 - accuracy: 0.4465 — val_f1: 0.4042\n",
      "\n",
      "Epoch 6: val_macro_f1 did not improve from 0.42231\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.9369 - accuracy: 0.4465 - val_loss: 0.9586 - val_accuracy: 0.4330 - val_macro_f1: 0.4042 - lr: 8.0000e-04\n",
      "Epoch 7/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.9026 - accuracy: 0.4677 — val_f1: 0.4454\n",
      "\n",
      "Epoch 7: val_macro_f1 improved from 0.42231 to 0.44539, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 152ms/step - loss: 0.9026 - accuracy: 0.4677 - val_loss: 0.8813 - val_accuracy: 0.4710 - val_macro_f1: 0.4454 - lr: 7.9508e-04\n",
      "Epoch 8/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.8870 - accuracy: 0.4849 — val_f1: 0.4602\n",
      "\n",
      "Epoch 8: val_macro_f1 improved from 0.44539 to 0.46024, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 153ms/step - loss: 0.8870 - accuracy: 0.4849 - val_loss: 0.8831 - val_accuracy: 0.4871 - val_macro_f1: 0.4602 - lr: 7.8045e-04\n",
      "Epoch 9/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.8972 - accuracy: 0.4737 — val_f1: 0.4610\n",
      "\n",
      "Epoch 9: val_macro_f1 improved from 0.46024 to 0.46101, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 155ms/step - loss: 0.8972 - accuracy: 0.4737 - val_loss: 0.8403 - val_accuracy: 0.4813 - val_macro_f1: 0.4610 - lr: 7.5646e-04\n",
      "Epoch 10/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.8792 - accuracy: 0.4961 — val_f1: 0.4887\n",
      "\n",
      "Epoch 10: val_macro_f1 improved from 0.46101 to 0.48867, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 153ms/step - loss: 0.8792 - accuracy: 0.4961 - val_loss: 0.8088 - val_accuracy: 0.5058 - val_macro_f1: 0.4887 - lr: 7.2370e-04\n",
      "Epoch 11/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7983 - accuracy: 0.5306 — val_f1: 0.5228\n",
      "\n",
      "Epoch 11: val_macro_f1 improved from 0.48867 to 0.52279, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 150ms/step - loss: 0.7983 - accuracy: 0.5306 - val_loss: 0.8297 - val_accuracy: 0.5335 - val_macro_f1: 0.5228 - lr: 6.8299e-04\n",
      "Epoch 12/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.8292 - accuracy: 0.5312 — val_f1: 0.5441\n",
      "\n",
      "Epoch 12: val_macro_f1 improved from 0.52279 to 0.54409, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 152ms/step - loss: 0.8292 - accuracy: 0.5312 - val_loss: 0.7878 - val_accuracy: 0.5470 - val_macro_f1: 0.5441 - lr: 6.3532e-04\n",
      "Epoch 13/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7671 - accuracy: 0.5626 — val_f1: 0.5231\n",
      "\n",
      "Epoch 13: val_macro_f1 did not improve from 0.54409\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.7671 - accuracy: 0.5626 - val_loss: 0.8227 - val_accuracy: 0.5367 - val_macro_f1: 0.5231 - lr: 5.8187e-04\n",
      "Epoch 14/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7593 - accuracy: 0.5795 — val_f1: 0.5592\n",
      "\n",
      "Epoch 14: val_macro_f1 improved from 0.54409 to 0.55923, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 153ms/step - loss: 0.7593 - accuracy: 0.5795 - val_loss: 0.7827 - val_accuracy: 0.5625 - val_macro_f1: 0.5592 - lr: 5.2395e-04\n",
      "Epoch 15/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7540 - accuracy: 0.5854 — val_f1: 0.5704\n",
      "\n",
      "Epoch 15: val_macro_f1 improved from 0.55923 to 0.57038, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 152ms/step - loss: 0.7540 - accuracy: 0.5854 - val_loss: 0.7371 - val_accuracy: 0.5715 - val_macro_f1: 0.5704 - lr: 4.6300e-04\n",
      "Epoch 16/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.6012 — val_f1: 0.5106\n",
      "\n",
      "Epoch 16: val_macro_f1 did not improve from 0.57038\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.7148 - accuracy: 0.6012 - val_loss: 0.7722 - val_accuracy: 0.5399 - val_macro_f1: 0.5106 - lr: 4.0050e-04\n",
      "Epoch 17/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7128 - accuracy: 0.6024 — val_f1: 0.5650\n",
      "\n",
      "Epoch 17: val_macro_f1 did not improve from 0.57038\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.7128 - accuracy: 0.6024 - val_loss: 0.7412 - val_accuracy: 0.5735 - val_macro_f1: 0.5650 - lr: 3.3800e-04\n",
      "Epoch 18/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6788 - accuracy: 0.6205 — val_f1: 0.5765\n",
      "\n",
      "Epoch 18: val_macro_f1 improved from 0.57038 to 0.57647, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 152ms/step - loss: 0.6788 - accuracy: 0.6205 - val_loss: 0.7425 - val_accuracy: 0.5831 - val_macro_f1: 0.5765 - lr: 2.7705e-04\n",
      "Epoch 19/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6688 - accuracy: 0.6374 — val_f1: 0.5959\n",
      "\n",
      "Epoch 19: val_macro_f1 improved from 0.57647 to 0.59591, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 151ms/step - loss: 0.6688 - accuracy: 0.6374 - val_loss: 0.7142 - val_accuracy: 0.5921 - val_macro_f1: 0.5959 - lr: 2.1913e-04\n",
      "Epoch 20/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6449 - accuracy: 0.6535 — val_f1: 0.5925\n",
      "\n",
      "Epoch 20: val_macro_f1 did not improve from 0.59591\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6449 - accuracy: 0.6535 - val_loss: 0.7130 - val_accuracy: 0.6024 - val_macro_f1: 0.5925 - lr: 1.6568e-04\n",
      "Epoch 21/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6452 - accuracy: 0.6626 — val_f1: 0.6208\n",
      "\n",
      "Epoch 21: val_macro_f1 improved from 0.59591 to 0.62079, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 151ms/step - loss: 0.6452 - accuracy: 0.6626 - val_loss: 0.6867 - val_accuracy: 0.6231 - val_macro_f1: 0.6208 - lr: 1.1801e-04\n",
      "Epoch 22/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6349 - accuracy: 0.6720 — val_f1: 0.6194\n",
      "\n",
      "Epoch 22: val_macro_f1 did not improve from 0.62079\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6349 - accuracy: 0.6720 - val_loss: 0.6838 - val_accuracy: 0.6237 - val_macro_f1: 0.6194 - lr: 7.7298e-05\n",
      "Epoch 23/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5758 - accuracy: 0.6927 — val_f1: 0.6234\n",
      "\n",
      "Epoch 23: val_macro_f1 improved from 0.62079 to 0.62338, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 153ms/step - loss: 0.5758 - accuracy: 0.6927 - val_loss: 0.6978 - val_accuracy: 0.6276 - val_macro_f1: 0.6234 - lr: 4.4543e-05\n",
      "Epoch 24/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6005 - accuracy: 0.6866 — val_f1: 0.6284\n",
      "\n",
      "Epoch 24: val_macro_f1 improved from 0.62338 to 0.62836, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 151ms/step - loss: 0.6005 - accuracy: 0.6866 - val_loss: 0.6828 - val_accuracy: 0.6314 - val_macro_f1: 0.6284 - lr: 2.0553e-05\n",
      "Epoch 25/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6108 - accuracy: 0.6801 — val_f1: 0.6271\n",
      "\n",
      "Epoch 25: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6108 - accuracy: 0.6801 - val_loss: 0.6859 - val_accuracy: 0.6302 - val_macro_f1: 0.6271 - lr: 5.9185e-06\n",
      "Epoch 26/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.6222 — val_f1: 0.3629\n",
      "\n",
      "Epoch 26: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6801 - accuracy: 0.6222 - val_loss: 1.2039 - val_accuracy: 0.3995 - val_macro_f1: 0.3629 - lr: 8.0000e-04\n",
      "Epoch 27/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7664 - accuracy: 0.5632 — val_f1: 0.5334\n",
      "\n",
      "Epoch 27: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.7664 - accuracy: 0.5632 - val_loss: 0.8717 - val_accuracy: 0.5335 - val_macro_f1: 0.5334 - lr: 7.9877e-04\n",
      "Epoch 28/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.7143 - accuracy: 0.6036 — val_f1: 0.5491\n",
      "\n",
      "Epoch 28: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.7143 - accuracy: 0.6036 - val_loss: 0.7421 - val_accuracy: 0.5657 - val_macro_f1: 0.5491 - lr: 7.9508e-04\n",
      "Epoch 29/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.6219 — val_f1: 0.5703\n",
      "\n",
      "Epoch 29: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6884 - accuracy: 0.6219 - val_loss: 0.7411 - val_accuracy: 0.5702 - val_macro_f1: 0.5703 - lr: 7.8896e-04\n",
      "Epoch 30/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6709 - accuracy: 0.6326 — val_f1: 0.5561\n",
      "\n",
      "Epoch 30: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6709 - accuracy: 0.6326 - val_loss: 0.7732 - val_accuracy: 0.5651 - val_macro_f1: 0.5561 - lr: 7.8045e-04\n",
      "Epoch 31/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.6329 — val_f1: 0.5766\n",
      "\n",
      "Epoch 31: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6866 - accuracy: 0.6329 - val_loss: 0.7677 - val_accuracy: 0.5773 - val_macro_f1: 0.5766 - lr: 7.6959e-04\n",
      "Epoch 32/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.6514 — val_f1: 0.5154\n",
      "\n",
      "Epoch 32: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6484 - accuracy: 0.6514 - val_loss: 0.7989 - val_accuracy: 0.5496 - val_macro_f1: 0.5154 - lr: 7.5646e-04\n",
      "Epoch 33/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.6652 — val_f1: 0.5946\n",
      "\n",
      "Epoch 33: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6217 - accuracy: 0.6652 - val_loss: 0.7260 - val_accuracy: 0.6005 - val_macro_f1: 0.5946 - lr: 7.4113e-04\n",
      "Epoch 34/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6601 - accuracy: 0.6380 — val_f1: 0.5690\n",
      "\n",
      "Epoch 34: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6601 - accuracy: 0.6380 - val_loss: 0.7979 - val_accuracy: 0.5889 - val_macro_f1: 0.5690 - lr: 7.2370e-04\n",
      "Epoch 35/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.6672 — val_f1: 0.6035\n",
      "\n",
      "Epoch 35: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6481 - accuracy: 0.6672 - val_loss: 0.7241 - val_accuracy: 0.6115 - val_macro_f1: 0.6035 - lr: 7.0428e-04\n",
      "Epoch 36/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.6786 — val_f1: 0.5724\n",
      "\n",
      "Epoch 36: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6240 - accuracy: 0.6786 - val_loss: 0.7763 - val_accuracy: 0.5754 - val_macro_f1: 0.5724 - lr: 6.8299e-04\n",
      "Epoch 37/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5745 - accuracy: 0.7019 — val_f1: 0.4976\n",
      "\n",
      "Epoch 37: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.5745 - accuracy: 0.7019 - val_loss: 0.9191 - val_accuracy: 0.5052 - val_macro_f1: 0.4976 - lr: 6.5995e-04\n",
      "Epoch 38/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5865 - accuracy: 0.7029 — val_f1: 0.6031\n",
      "\n",
      "Epoch 38: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.5865 - accuracy: 0.7029 - val_loss: 0.6989 - val_accuracy: 0.6147 - val_macro_f1: 0.6031 - lr: 6.3532e-04\n",
      "Epoch 39/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6081 - accuracy: 0.6879 — val_f1: 0.5653\n",
      "\n",
      "Epoch 39: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.6081 - accuracy: 0.6879 - val_loss: 0.7651 - val_accuracy: 0.5696 - val_macro_f1: 0.5653 - lr: 6.0924e-04\n",
      "Epoch 40/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5931 - accuracy: 0.6993 — val_f1: 0.6225\n",
      "\n",
      "Epoch 40: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.5931 - accuracy: 0.6993 - val_loss: 0.6862 - val_accuracy: 0.6224 - val_macro_f1: 0.6225 - lr: 5.8187e-04\n",
      "Epoch 41/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.7234 — val_f1: 0.5828\n",
      "\n",
      "Epoch 41: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 150ms/step - loss: 0.5466 - accuracy: 0.7234 - val_loss: 0.7741 - val_accuracy: 0.5889 - val_macro_f1: 0.5828 - lr: 5.5338e-04\n",
      "Epoch 42/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5586 - accuracy: 0.7255 — val_f1: 0.5539\n",
      "\n",
      "Epoch 42: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 146ms/step - loss: 0.5586 - accuracy: 0.7255 - val_loss: 0.8337 - val_accuracy: 0.5567 - val_macro_f1: 0.5539 - lr: 5.2395e-04\n",
      "Epoch 43/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.7533 — val_f1: 0.6028\n",
      "\n",
      "Epoch 43: val_macro_f1 did not improve from 0.62836\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.5096 - accuracy: 0.7533 - val_loss: 0.7428 - val_accuracy: 0.6121 - val_macro_f1: 0.6028 - lr: 4.9376e-04\n",
      "Epoch 44/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5175 - accuracy: 0.7504 — val_f1: 0.6323\n",
      "\n",
      "Epoch 44: val_macro_f1 improved from 0.62836 to 0.63231, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 152ms/step - loss: 0.5175 - accuracy: 0.7504 - val_loss: 0.7192 - val_accuracy: 0.6353 - val_macro_f1: 0.6323 - lr: 4.6300e-04\n",
      "Epoch 45/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.7360 — val_f1: 0.6234\n",
      "\n",
      "Epoch 45: val_macro_f1 did not improve from 0.63231\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.5338 - accuracy: 0.7360 - val_loss: 0.6877 - val_accuracy: 0.6269 - val_macro_f1: 0.6234 - lr: 4.3184e-04\n",
      "Epoch 46/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.7662 — val_f1: 0.6176\n",
      "\n",
      "Epoch 46: val_macro_f1 did not improve from 0.63231\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.4792 - accuracy: 0.7662 - val_loss: 0.7162 - val_accuracy: 0.6198 - val_macro_f1: 0.6176 - lr: 4.0050e-04\n",
      "Epoch 47/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5027 - accuracy: 0.7620 — val_f1: 0.6257\n",
      "\n",
      "Epoch 47: val_macro_f1 did not improve from 0.63231\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.5027 - accuracy: 0.7620 - val_loss: 0.6867 - val_accuracy: 0.6269 - val_macro_f1: 0.6257 - lr: 3.6916e-04\n",
      "Epoch 48/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4897 - accuracy: 0.7803 — val_f1: 0.6263\n",
      "\n",
      "Epoch 48: val_macro_f1 did not improve from 0.63231\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.4897 - accuracy: 0.7803 - val_loss: 0.7040 - val_accuracy: 0.6327 - val_macro_f1: 0.6263 - lr: 3.3800e-04\n",
      "Epoch 49/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4423 - accuracy: 0.7969 — val_f1: 0.6012\n",
      "\n",
      "Epoch 49: val_macro_f1 did not improve from 0.63231\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.4423 - accuracy: 0.7969 - val_loss: 0.7514 - val_accuracy: 0.6108 - val_macro_f1: 0.6012 - lr: 3.0724e-04\n",
      "Epoch 50/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4822 - accuracy: 0.7739 — val_f1: 0.6453\n",
      "\n",
      "Epoch 50: val_macro_f1 improved from 0.63231 to 0.64528, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 151ms/step - loss: 0.4822 - accuracy: 0.7739 - val_loss: 0.6758 - val_accuracy: 0.6443 - val_macro_f1: 0.6453 - lr: 2.7705e-04\n",
      "Epoch 51/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.8175 — val_f1: 0.6541\n",
      "\n",
      "Epoch 51: val_macro_f1 improved from 0.64528 to 0.65411, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 150ms/step - loss: 0.4299 - accuracy: 0.8175 - val_loss: 0.6655 - val_accuracy: 0.6553 - val_macro_f1: 0.6541 - lr: 2.4762e-04\n",
      "Epoch 52/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4254 - accuracy: 0.8295 — val_f1: 0.6486\n",
      "\n",
      "Epoch 52: val_macro_f1 did not improve from 0.65411\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4254 - accuracy: 0.8295 - val_loss: 0.6896 - val_accuracy: 0.6508 - val_macro_f1: 0.6486 - lr: 2.1913e-04\n",
      "Epoch 53/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.8309 — val_f1: 0.6527\n",
      "\n",
      "Epoch 53: val_macro_f1 did not improve from 0.65411\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4027 - accuracy: 0.8309 - val_loss: 0.6716 - val_accuracy: 0.6566 - val_macro_f1: 0.6527 - lr: 1.9176e-04\n",
      "Epoch 54/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4050 - accuracy: 0.8360 — val_f1: 0.6544\n",
      "\n",
      "Epoch 54: val_macro_f1 improved from 0.65411 to 0.65445, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 151ms/step - loss: 0.4050 - accuracy: 0.8360 - val_loss: 0.6718 - val_accuracy: 0.6546 - val_macro_f1: 0.6544 - lr: 1.6568e-04\n",
      "Epoch 55/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.8416 — val_f1: 0.6408\n",
      "\n",
      "Epoch 55: val_macro_f1 did not improve from 0.65445\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.4117 - accuracy: 0.8416 - val_loss: 0.7007 - val_accuracy: 0.6450 - val_macro_f1: 0.6408 - lr: 1.4105e-04\n",
      "Epoch 56/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.8615 — val_f1: 0.6545\n",
      "\n",
      "Epoch 56: val_macro_f1 improved from 0.65445 to 0.65455, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 152ms/step - loss: 0.3880 - accuracy: 0.8615 - val_loss: 0.6858 - val_accuracy: 0.6559 - val_macro_f1: 0.6545 - lr: 1.1801e-04\n",
      "Epoch 57/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4044 - accuracy: 0.8572 — val_f1: 0.6628\n",
      "\n",
      "Epoch 57: val_macro_f1 improved from 0.65455 to 0.66282, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 151ms/step - loss: 0.4044 - accuracy: 0.8572 - val_loss: 0.6797 - val_accuracy: 0.6649 - val_macro_f1: 0.6628 - lr: 9.6718e-05\n",
      "Epoch 58/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8423 — val_f1: 0.6628\n",
      "\n",
      "Epoch 58: val_macro_f1 improved from 0.66282 to 0.66283, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 152ms/step - loss: 0.4277 - accuracy: 0.8423 - val_loss: 0.6778 - val_accuracy: 0.6643 - val_macro_f1: 0.6628 - lr: 7.7298e-05\n",
      "Epoch 59/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.8642 — val_f1: 0.6541\n",
      "\n",
      "Epoch 59: val_macro_f1 did not improve from 0.66283\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.3848 - accuracy: 0.8642 - val_loss: 0.6952 - val_accuracy: 0.6566 - val_macro_f1: 0.6541 - lr: 5.9870e-05\n",
      "Epoch 60/120\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3899 - accuracy: 0.8626 — val_f1: 0.6612\n",
      "\n",
      "Epoch 60: val_macro_f1 did not improve from 0.66283\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.3899 - accuracy: 0.8625 - val_loss: 0.6778 - val_accuracy: 0.6630 - val_macro_f1: 0.6612 - lr: 4.4543e-05\n",
      "Epoch 61/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.8626 — val_f1: 0.6624\n",
      "\n",
      "Epoch 61: val_macro_f1 did not improve from 0.66283\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.3832 - accuracy: 0.8626 - val_loss: 0.6828 - val_accuracy: 0.6637 - val_macro_f1: 0.6624 - lr: 3.1410e-05\n",
      "Epoch 62/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3390 - accuracy: 0.8774 — val_f1: 0.6599\n",
      "\n",
      "Epoch 62: val_macro_f1 did not improve from 0.66283\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.3390 - accuracy: 0.8774 - val_loss: 0.6924 - val_accuracy: 0.6617 - val_macro_f1: 0.6599 - lr: 2.0553e-05\n",
      "Epoch 63/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4232 - accuracy: 0.8521 — val_f1: 0.6641\n",
      "\n",
      "Epoch 63: val_macro_f1 improved from 0.66283 to 0.66409, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 151ms/step - loss: 0.4232 - accuracy: 0.8521 - val_loss: 0.6823 - val_accuracy: 0.6662 - val_macro_f1: 0.6641 - lr: 1.2038e-05\n",
      "Epoch 64/120\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8568 — val_f1: 0.6643\n",
      "\n",
      "Epoch 64: val_macro_f1 improved from 0.66409 to 0.66434, saving model to Output/conformer_v3_2_notebook_20251204_064339/models/best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 14s 152ms/step - loss: 0.3988 - accuracy: 0.8569 - val_loss: 0.6783 - val_accuracy: 0.6662 - val_macro_f1: 0.6643 - lr: 5.9185e-06\n",
      "Epoch 65/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.8596 — val_f1: 0.6642\n",
      "\n",
      "Epoch 65: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.3805 - accuracy: 0.8596 - val_loss: 0.6783 - val_accuracy: 0.6662 - val_macro_f1: 0.6642 - lr: 2.2315e-06\n",
      "Epoch 66/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6009 - accuracy: 0.6796 — val_f1: 0.5338\n",
      "\n",
      "Epoch 66: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.6009 - accuracy: 0.6796 - val_loss: 0.8869 - val_accuracy: 0.5393 - val_macro_f1: 0.5338 - lr: 8.0000e-04\n",
      "Epoch 67/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.6353 - accuracy: 0.6825 — val_f1: 0.5729\n",
      "\n",
      "Epoch 67: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.6353 - accuracy: 0.6825 - val_loss: 0.8133 - val_accuracy: 0.5818 - val_macro_f1: 0.5729 - lr: 7.9969e-04\n",
      "Epoch 68/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.7250 — val_f1: 0.5359\n",
      "\n",
      "Epoch 68: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 145ms/step - loss: 0.5676 - accuracy: 0.7250 - val_loss: 0.8518 - val_accuracy: 0.5432 - val_macro_f1: 0.5359 - lr: 7.9877e-04\n",
      "Epoch 69/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5185 - accuracy: 0.7463 — val_f1: 0.5793\n",
      "\n",
      "Epoch 69: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.5185 - accuracy: 0.7463 - val_loss: 0.8052 - val_accuracy: 0.5857 - val_macro_f1: 0.5793 - lr: 7.9723e-04\n",
      "Epoch 70/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.7509 — val_f1: 0.6147\n",
      "\n",
      "Epoch 70: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.5380 - accuracy: 0.7509 - val_loss: 0.7216 - val_accuracy: 0.6134 - val_macro_f1: 0.6147 - lr: 7.9508e-04\n",
      "Epoch 71/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.7699 — val_f1: 0.6196\n",
      "\n",
      "Epoch 71: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4982 - accuracy: 0.7699 - val_loss: 0.6931 - val_accuracy: 0.6198 - val_macro_f1: 0.6196 - lr: 7.9232e-04\n",
      "Epoch 72/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5469 - accuracy: 0.7307 — val_f1: 0.5438\n",
      "\n",
      "Epoch 72: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.5469 - accuracy: 0.7307 - val_loss: 0.8190 - val_accuracy: 0.5535 - val_macro_f1: 0.5438 - lr: 7.8896e-04\n",
      "Epoch 73/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.7686 — val_f1: 0.6022\n",
      "\n",
      "Epoch 73: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.5134 - accuracy: 0.7686 - val_loss: 0.7512 - val_accuracy: 0.6108 - val_macro_f1: 0.6022 - lr: 7.8500e-04\n",
      "Epoch 74/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.7881 — val_f1: 0.5440\n",
      "\n",
      "Epoch 74: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4645 - accuracy: 0.7881 - val_loss: 0.8603 - val_accuracy: 0.5457 - val_macro_f1: 0.5440 - lr: 7.8045e-04\n",
      "Epoch 75/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.7954 — val_f1: 0.5832\n",
      "\n",
      "Epoch 75: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.4741 - accuracy: 0.7954 - val_loss: 0.7833 - val_accuracy: 0.5934 - val_macro_f1: 0.5832 - lr: 7.7531e-04\n",
      "Epoch 76/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.7660 — val_f1: 0.6288\n",
      "\n",
      "Epoch 76: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4885 - accuracy: 0.7660 - val_loss: 0.7435 - val_accuracy: 0.6314 - val_macro_f1: 0.6288 - lr: 7.6959e-04\n",
      "Epoch 77/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.7992 — val_f1: 0.5934\n",
      "\n",
      "Epoch 77: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.4589 - accuracy: 0.7992 - val_loss: 0.7998 - val_accuracy: 0.5947 - val_macro_f1: 0.5934 - lr: 7.6330e-04\n",
      "Epoch 78/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4423 - accuracy: 0.8199 — val_f1: 0.5979\n",
      "\n",
      "Epoch 78: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.4423 - accuracy: 0.8199 - val_loss: 0.7856 - val_accuracy: 0.5973 - val_macro_f1: 0.5979 - lr: 7.5646e-04\n",
      "Epoch 79/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4285 - accuracy: 0.8058 — val_f1: 0.5806\n",
      "\n",
      "Epoch 79: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4285 - accuracy: 0.8058 - val_loss: 0.8469 - val_accuracy: 0.5851 - val_macro_f1: 0.5806 - lr: 7.4906e-04\n",
      "Epoch 80/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.8098 — val_f1: 0.6035\n",
      "\n",
      "Epoch 80: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.4340 - accuracy: 0.8098 - val_loss: 0.7486 - val_accuracy: 0.6140 - val_macro_f1: 0.6035 - lr: 7.4113e-04\n",
      "Epoch 81/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.8170 — val_f1: 0.6365\n",
      "\n",
      "Epoch 81: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 146ms/step - loss: 0.4173 - accuracy: 0.8170 - val_loss: 0.7444 - val_accuracy: 0.6372 - val_macro_f1: 0.6365 - lr: 7.3267e-04\n",
      "Epoch 82/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.8474 — val_f1: 0.6093\n",
      "\n",
      "Epoch 82: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.3817 - accuracy: 0.8474 - val_loss: 0.8048 - val_accuracy: 0.6205 - val_macro_f1: 0.6093 - lr: 7.2370e-04\n",
      "Epoch 83/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.8480 — val_f1: 0.5712\n",
      "\n",
      "Epoch 83: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.3770 - accuracy: 0.8480 - val_loss: 0.8625 - val_accuracy: 0.5683 - val_macro_f1: 0.5712 - lr: 7.1423e-04\n",
      "Epoch 84/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4181 - accuracy: 0.8416 — val_f1: 0.6267\n",
      "\n",
      "Epoch 84: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4181 - accuracy: 0.8416 - val_loss: 0.7472 - val_accuracy: 0.6314 - val_macro_f1: 0.6267 - lr: 7.0428e-04\n",
      "Epoch 85/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4304 - accuracy: 0.8377 — val_f1: 0.5675\n",
      "\n",
      "Epoch 85: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.4304 - accuracy: 0.8377 - val_loss: 0.8289 - val_accuracy: 0.5857 - val_macro_f1: 0.5675 - lr: 6.9386e-04\n",
      "Epoch 86/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4376 - accuracy: 0.8222 — val_f1: 0.6025\n",
      "\n",
      "Epoch 86: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4376 - accuracy: 0.8222 - val_loss: 0.7472 - val_accuracy: 0.6108 - val_macro_f1: 0.6025 - lr: 6.8299e-04\n",
      "Epoch 87/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.8216 — val_f1: 0.5969\n",
      "\n",
      "Epoch 87: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4318 - accuracy: 0.8216 - val_loss: 0.8427 - val_accuracy: 0.6037 - val_macro_f1: 0.5969 - lr: 6.7168e-04\n",
      "Epoch 88/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8370 — val_f1: 0.6184\n",
      "\n",
      "Epoch 88: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.4206 - accuracy: 0.8370 - val_loss: 0.7715 - val_accuracy: 0.6173 - val_macro_f1: 0.6184 - lr: 6.5995e-04\n",
      "Epoch 89/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3946 - accuracy: 0.8542 — val_f1: 0.6228\n",
      "\n",
      "Epoch 89: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.3946 - accuracy: 0.8542 - val_loss: 0.7839 - val_accuracy: 0.6289 - val_macro_f1: 0.6228 - lr: 6.4783e-04\n",
      "Epoch 90/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.4961 - accuracy: 0.7832 — val_f1: 0.6066\n",
      "\n",
      "Epoch 90: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 0.4961 - accuracy: 0.7832 - val_loss: 0.7711 - val_accuracy: 0.6140 - val_macro_f1: 0.6066 - lr: 6.3532e-04\n",
      "Epoch 91/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8584 — val_f1: 0.6454\n",
      "\n",
      "Epoch 91: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 149ms/step - loss: 0.3953 - accuracy: 0.8584 - val_loss: 0.7306 - val_accuracy: 0.6443 - val_macro_f1: 0.6454 - lr: 6.2245e-04\n",
      "Epoch 92/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.8525 — val_f1: 0.6136\n",
      "\n",
      "Epoch 92: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.3975 - accuracy: 0.8525 - val_loss: 0.8001 - val_accuracy: 0.6218 - val_macro_f1: 0.6136 - lr: 6.0924e-04\n",
      "Epoch 93/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8774 — val_f1: 0.6131\n",
      "\n",
      "Epoch 93: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.3527 - accuracy: 0.8774 - val_loss: 0.7731 - val_accuracy: 0.6192 - val_macro_f1: 0.6131 - lr: 5.9570e-04\n",
      "Epoch 94/120\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.3587 - accuracy: 0.8883 — val_f1: 0.5338\n",
      "\n",
      "Epoch 94: val_macro_f1 did not improve from 0.66434\n",
      "93/93 [==============================] - 14s 148ms/step - loss: 0.3587 - accuracy: 0.8883 - val_loss: 0.9181 - val_accuracy: 0.5380 - val_macro_f1: 0.5338 - lr: 5.8187e-04\n",
      "\n",
      "Evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG     0.6818    0.8491    0.7563       265\n",
      "         DIS     0.6905    0.5472    0.6105       265\n",
      "         FEA     0.6960    0.5962    0.6423       265\n",
      "         HAP     0.6357    0.6189    0.6272       265\n",
      "         NEU     0.7008    0.7841    0.7401       227\n",
      "         SAD     0.6007    0.6189    0.6097       265\n",
      "\n",
      "    accuracy                         0.6662      1552\n",
      "   macro avg     0.6676    0.6691    0.6643      1552\n",
      "weighted avg     0.6668    0.6662    0.6625      1552\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 语音情绪识别 - Conformer V3.2 (High-Res + Large Kernel + SGDR)\n",
    "# 数据集: CREMA-D\n",
    "# 改进:\n",
    "# 1. N_MELS 64 -> 80 (更高分辨率)\n",
    "# 2. Kernel Size 15 -> 31 (针对悲伤情绪的大感受野)\n",
    "# 3. LR Schedule -> Cosine with Restarts (跳出局部最优)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# =====================\n",
    "# 1) 自动创建输出目录\n",
    "# =====================\n",
    "def create_experiment_dir(base_dir=\"Output\"):\n",
    "    try:\n",
    "        script_name = os.path.splitext(os.path.basename(__file__))[0]\n",
    "    except NameError:\n",
    "        script_name = \"conformer_v3_2_notebook\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    exp_dir = os.path.join(base_dir, f\"{script_name}_{timestamp}\")\n",
    "    \n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    for subdir in [\"models\", \"plots\", \"logs\"]:\n",
    "        os.makedirs(os.path.join(exp_dir, subdir), exist_ok=True)\n",
    "    \n",
    "    print(f\"✓ Experiment directory: {exp_dir}\")\n",
    "    return exp_dir\n",
    "\n",
    "EXP_DIR = create_experiment_dir()\n",
    "MODEL_DIR = os.path.join(EXP_DIR, \"models\")\n",
    "PLOT_DIR = os.path.join(EXP_DIR, \"plots\")\n",
    "LOG_DIR = os.path.join(EXP_DIR, \"logs\")\n",
    "\n",
    "# =====================\n",
    "# 2) 数据路径\n",
    "# =====================\n",
    "AUDIO_DIR = Path(\"../AudioWAV\")\n",
    "assert AUDIO_DIR.exists(), f\"Not found: {AUDIO_DIR}\"\n",
    "\n",
    "# =====================\n",
    "# 3) 元数据\n",
    "# =====================\n",
    "def build_metadata(audio_dir):\n",
    "    records = []\n",
    "    for wav_file in audio_dir.glob(\"*.wav\"):\n",
    "        parts = wav_file.stem.split(\"_\")\n",
    "        if len(parts) == 4:\n",
    "            records.append({\n",
    "                \"path\": wav_file,\n",
    "                \"speaker\": parts[0],\n",
    "                \"emotion\": parts[2]\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "meta = build_metadata(AUDIO_DIR)\n",
    "print(f\"Samples: {len(meta)}\")\n",
    "\n",
    "# =====================\n",
    "# 4) 特征提取配置 (V3.2 升级点: N_MELS 80)\n",
    "# =====================\n",
    "SR = 16000\n",
    "N_MELS = 80          # 升级: 64 -> 80\n",
    "FFT = 1024\n",
    "HOP = 160\n",
    "WIN = 400\n",
    "FIXED_SECONDS = 3.0\n",
    "MAX_FRAMES = int(math.ceil(FIXED_SECONDS * SR / HOP))\n",
    "\n",
    "print(f\"Feature shape: ({MAX_FRAMES}, {N_MELS}, 1)\")\n",
    "\n",
    "# =====================\n",
    "# 5) 特征提取函数\n",
    "# =====================\n",
    "def load_logmel(path: Path):\n",
    "    y, _ = librosa.load(path, sr=SR, mono=True)\n",
    "    target_len = int(FIXED_SECONDS * SR)\n",
    "    if len(y) < target_len: y = np.pad(y, (0, target_len - len(y)))\n",
    "    else: y = y[:target_len]\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(y=y, sr=SR, n_fft=FFT, hop_length=HOP, win_length=WIN, n_mels=N_MELS)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max).astype(np.float32)\n",
    "    feat = np.transpose(S_db[..., None], (1, 0, 2))\n",
    "    \n",
    "    if feat.shape[0] < MAX_FRAMES:\n",
    "        feat = np.pad(feat, ((0, MAX_FRAMES - feat.shape[0]), (0, 0), (0, 0)))\n",
    "    else: feat = feat[:MAX_FRAMES]\n",
    "    return feat\n",
    "\n",
    "# 批量提取\n",
    "print(\"Extracting features (80 Mels)...\")\n",
    "specs, emotions, speakers = [], [], []\n",
    "for idx, row in meta.iterrows():\n",
    "    specs.append(load_logmel(row['path']))\n",
    "    emotions.append(row['emotion'])\n",
    "    speakers.append(row['speaker'])\n",
    "\n",
    "X = np.stack(specs)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "\n",
    "# =====================\n",
    "# 6) 划分与归一化\n",
    "# =====================\n",
    "le_emo = LabelEncoder()\n",
    "y = le_emo.fit_transform(emotions).astype(np.int32)\n",
    "groups = np.array(speakers)\n",
    "n_classes = len(le_emo.classes_)\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "Xtr, Xte = X[train_idx], X[test_idx]\n",
    "ytr, yte = y[train_idx], y[test_idx]\n",
    "\n",
    "# Z-Score\n",
    "mean_spec = Xtr.mean(axis=(0, 1, 2), keepdims=True)\n",
    "std_spec = Xtr.std(axis=(0, 1, 2), keepdims=True) + 1e-6\n",
    "Xtr = (Xtr - mean_spec) / std_spec\n",
    "Xte = (Xte - mean_spec) / std_spec\n",
    "\n",
    "# 保存参数\n",
    "np.save(os.path.join(MODEL_DIR, \"norm_mean.npy\"), mean_spec)\n",
    "np.save(os.path.join(MODEL_DIR, \"norm_std.npy\"), std_spec)\n",
    "\n",
    "# =====================\n",
    "# 7) 组件定义\n",
    "# =====================\n",
    "class SpecAugment(layers.Layer):\n",
    "    def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "    def call(self, x, training=False):\n",
    "        if not training: return x\n",
    "        B, T, F = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
    "        # 简单版实现\n",
    "        # Time Mask\n",
    "        t = tf.random.uniform([], 0, 20, dtype=tf.int32)\n",
    "        t0 = tf.random.uniform([], 0, tf.maximum(1, T-t), dtype=tf.int32)\n",
    "        m1 = tf.concat([tf.ones([B, t0, F, 1]), tf.zeros([B, t, F, 1]), tf.ones([B, T-t0-t, F, 1])], 1)\n",
    "        # Freq Mask\n",
    "        f = tf.random.uniform([], 0, 10, dtype=tf.int32)\n",
    "        f0 = tf.random.uniform([], 0, tf.maximum(1, F-f), dtype=tf.int32)\n",
    "        m2 = tf.concat([tf.ones([B, T, f0, 1]), tf.zeros([B, T, f, 1]), tf.ones([B, T, F-f0-f, 1])], 2)\n",
    "        return x * m1 * m2\n",
    "\n",
    "class LabelSmoothingFocalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, smoothing=0.05, gamma=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.smoothing = smoothing\n",
    "        self.gamma = gamma\n",
    "    def call(self, y_true, y_pred):\n",
    "        n = tf.cast(tf.shape(y_pred)[-1], tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_true = y_true * (1 - self.smoothing) + self.smoothing / n\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        return tf.reduce_sum(-y_true * tf.math.log(y_pred) * tf.pow(1 - y_pred, self.gamma), axis=-1)\n",
    "\n",
    "def glu(x): return x[..., :x.shape[-1]//2] * tf.sigmoid(x[..., x.shape[-1]//2:])\n",
    "\n",
    "def se_block(x, ratio=8):\n",
    "    ch = x.shape[-1]\n",
    "    s = layers.GlobalAveragePooling2D()(x)\n",
    "    s = layers.Dense(ch//ratio, activation='relu')(s)\n",
    "    s = layers.Dense(ch, activation='sigmoid')(s)\n",
    "    return layers.Multiply()([x, layers.Reshape((1,1,ch))(s)])\n",
    "\n",
    "def attentive_pooling(x):\n",
    "    a = layers.Dense(1, activation='tanh')(x)\n",
    "    a = layers.Softmax(axis=1)(a)\n",
    "    return tf.reduce_sum(x * a, axis=1)\n",
    "\n",
    "# =====================\n",
    "# 8) 架构升级\n",
    "# =====================\n",
    "def multi_scale_conv_block(x):\n",
    "    # L2 正则 V3.1 保持 5e-5\n",
    "    reg = tf.keras.regularizers.l2(5e-5)\n",
    "    c3 = layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
    "    c5 = layers.Conv2D(32, 5, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
    "    c7 = layers.Conv2D(32, 7, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
    "    x = layers.Concatenate()([c3, c5, c7])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = se_block(x)\n",
    "    return layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "def conformer_block(x, d_model=128, heads=4, kernel_size=31, dropout=0.15):\n",
    "    # V3.2 升级点: kernel_size 默认为 31 (大感受野)\n",
    "    r = x\n",
    "    x = layers.Dense(d_model*4, activation='swish')(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(d_model)(x)\n",
    "    x = layers.LayerNormalization()(r + 0.5*x) # FFN 1\n",
    "    \n",
    "    r = x\n",
    "    x = layers.MultiHeadAttention(heads, d_model//heads, dropout=dropout)(x, x)\n",
    "    x = layers.LayerNormalization()(r + x) # MHSA\n",
    "    \n",
    "    r = x\n",
    "    x = layers.Conv1D(d_model*2, 1)(x)\n",
    "    x = layers.Lambda(glu)(x)\n",
    "    # Depthwise Conv with Large Kernel\n",
    "    x = layers.DepthwiseConv1D(kernel_size, padding='same')(x) \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    x = layers.Conv1D(d_model, 1)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization()(r + x) # Conv\n",
    "    \n",
    "    r = x\n",
    "    x = layers.Dense(d_model*4, activation='swish')(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(d_model)(x)\n",
    "    x = layers.LayerNormalization()(r + 0.5*x) # FFN 2\n",
    "    return x\n",
    "\n",
    "def build_conformer_v3_2(input_shape, n_classes):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.GaussianNoise(0.02)(inp)\n",
    "    x = SpecAugment()(x)\n",
    "    \n",
    "    # CNN Front (96 -> 128 -> 256 channels)\n",
    "    x = multi_scale_conv_block(x)\n",
    "    \n",
    "    reg = tf.keras.regularizers.l2(5e-5)\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x) \n",
    "    x = se_block(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = se_block(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "    \n",
    "    # Conformer\n",
    "    # 将 (B, T, F, C) -> (B, T, F*C)\n",
    "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
    "    \n",
    "    x = layers.Dense(144)(x) # 适应 N_MELS 80\n",
    "    x = layers.Dropout(0.15)(x)\n",
    "    \n",
    "    # Block 1: Kernel 15 (Local)\n",
    "    x = conformer_block(x, d_model=144, kernel_size=15)\n",
    "    # Block 2: Kernel 31 (Global/Long-term)\n",
    "    x = conformer_block(x, d_model=144, kernel_size=31)\n",
    "    \n",
    "    x = attentive_pooling(x)\n",
    "    \n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=reg)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(n_classes, activation='softmax', name='emotion')(x)\n",
    "    \n",
    "    return models.Model(inp, out)\n",
    "\n",
    "# =====================\n",
    "# 9) 训练循环 (带重启)\n",
    "# =====================\n",
    "class MixupGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x, y, batch=64):\n",
    "        self.x, self.y, self.batch = x, y, batch\n",
    "        self.idx = np.arange(len(x))\n",
    "        np.random.shuffle(self.idx)\n",
    "    def __len__(self): return int(np.ceil(len(self.x)/self.batch))\n",
    "    def __getitem__(self, i):\n",
    "        inds = self.idx[i*self.batch:(i+1)*self.batch]\n",
    "        bx, by = self.x[inds].copy(), self.y[inds].copy()\n",
    "        if np.random.random()<0.5:\n",
    "            lam = np.random.beta(0.2, 0.2)\n",
    "            perm = np.random.permutation(len(bx))\n",
    "            bx = lam*bx + (1-lam)*bx[perm]\n",
    "            by = lam*by + (1-lam)*by[perm]\n",
    "        return bx, by\n",
    "    def on_epoch_end(self): np.random.shuffle(self.idx)\n",
    "\n",
    "# Cosine Decay with Restarts Scheduler\n",
    "# 周期: 10, 20, 40, 80 epochs (倍增)\n",
    "def sgdr_schedule(epoch):\n",
    "    lr_min = 1e-6\n",
    "    lr_max = 8e-4\n",
    "    \n",
    "    # Warmup\n",
    "    if epoch < 5:\n",
    "        return lr_max * (epoch + 1) / 5\n",
    "    \n",
    "    # SGDR logic\n",
    "    epoch = epoch - 5\n",
    "    cycle_len = 20  # 周期长度\n",
    "    mult = 2        # 周期倍增因子\n",
    "    \n",
    "    # 找出当前在哪个周期\n",
    "    cycle_idx = 0\n",
    "    while epoch >= cycle_len:\n",
    "        epoch -= cycle_len\n",
    "        cycle_len *= mult\n",
    "        cycle_idx += 1\n",
    "        \n",
    "    # Cosine decay within cycle\n",
    "    return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(np.pi * epoch / cycle_len))\n",
    "\n",
    "# 编译与训练\n",
    "model = build_conformer_v3_2(Xtr.shape[1:], n_classes)\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=8e-4, weight_decay=2e-4),\n",
    "              loss=LabelSmoothingFocalLoss(smoothing=0.05), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "class F1Cb(callbacks.Callback):\n",
    "    def on_epoch_end(self, e, logs):\n",
    "        p = np.argmax(self.model.predict(Xte, verbose=0), axis=1)\n",
    "        f1 = f1_score(yte, p, average='macro')\n",
    "        print(f\" — val_f1: {f1:.4f}\")\n",
    "        logs['val_macro_f1'] = f1\n",
    "\n",
    "callbacks_list = [\n",
    "    F1Cb(),\n",
    "    callbacks.ModelCheckpoint(os.path.join(MODEL_DIR, \"best.h5\"), monitor='val_macro_f1', mode='max', save_best_only=True, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor='val_macro_f1', mode='max', patience=30, restore_best_weights=True),\n",
    "    callbacks.LearningRateScheduler(sgdr_schedule),\n",
    "    callbacks.CSVLogger(os.path.join(LOG_DIR, \"log.csv\"))\n",
    "]\n",
    "\n",
    "print(f\"\\nStarting V3.2 Training (SGDR + Large Kernel)...\")\n",
    "history = model.fit(\n",
    "    MixupGen(Xtr, tf.keras.utils.to_categorical(ytr), 64),\n",
    "    validation_data=(Xte, tf.keras.utils.to_categorical(yte)),\n",
    "    epochs=120, # 增加Epoch数以适应重启策略\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# 10) 评估\n",
    "# =====================\n",
    "print(\"\\nEvaluating...\")\n",
    "p = np.argmax(model.predict(Xte, verbose=0), axis=1)\n",
    "print(classification_report(yte, p, target_names=le_emo.classes_, digits=4))\n",
    "\n",
    "# Plot LR\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history.history['lr'])\n",
    "plt.title(\"SGDR Learning Rate Schedule\")\n",
    "plt.savefig(os.path.join(PLOT_DIR, \"lr_schedule.png\"))\n",
    "plt.close()\n",
    "\n",
    "cm = confusion_matrix(yte, p)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le_emo.classes_, yticklabels=le_emo.classes_)\n",
    "plt.title(\"Confusion Matrix (V3.2)\")\n",
    "plt.savefig(os.path.join(PLOT_DIR, \"cm.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909d5be-43eb-478f-b213-0f16494bded3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
